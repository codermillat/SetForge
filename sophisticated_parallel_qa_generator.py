#!/usr/bin/env python3
"""
ðŸš€ SOPHISTICATED PARALLEL Q&A GENERATOR WITH QUALITY ASSURANCE
================================================================

Advanced production system that generates Q&A pairs in parallel while 
performing real-time quality checking and validation. Features:

âœ¨ Key Features:
1. ðŸ”„ Parallel Q&A Generation (Multi-threaded processing)
2. ðŸŽ¯ Real-time Quality Assurance (Inline validation)
3. ðŸ“Š Adaptive Quality Control (Dynamic threshold adjustment)
4. ðŸ” Intelligent Retry Logic (Auto-improvement)
5. ðŸ“ˆ Live Quality Monitoring (Real-time metrics)
6. ðŸŽ¨ Diverse Content Generation (Multiple strategies)
7. ðŸŒŸ Cultural Integration (Bengali-English seamless)
8. ðŸ”’ Quality Guarantee (Only high-quality output)

ðŸŽ¯ Quality Targets:
- Extractive Score: â‰¥0.85 (vs 0.75 standard)
- Factual Accuracy: â‰¥0.90 (vs 0.80 standard)
- Cultural Sensitivity: â‰¥0.85 (vs 0.70 standard)
- Uniqueness Score: â‰¥0.80 (vs 0.70 standard)
- Semantic Alignment: â‰¥0.95 (vs 0.85 standard)
"""

import asyncio
import concurrent.futures
import json
import logging
import re
import random
import time
import threading
from dataclasses import dataclass, asdict
from enum import Enum
from pathlib import Path
from typing import Dict, List, Any, Optional, Set, Tuple, Union
from datetime import datetime
import argparse
from collections import defaultdict, deque
import statistics

# Configure sophisticated logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('output/logs/sophisticated_qa_generator.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class QualityLevel(Enum):
    EXCELLENT = "excellent"  # All metrics above target
    GOOD = "good"           # Most metrics above target
    FAIR = "fair"           # Some metrics above target
    POOR = "poor"           # Below minimum thresholds

class GenerationStrategy(Enum):
    EXTRACTIVE_FOCUSED = "extractive_focused"
    CULTURAL_ENHANCED = "cultural_enhanced"
    COMPARATIVE_ANALYSIS = "comparative_analysis"
    DETAILED_GUIDANCE = "detailed_guidance"
    QUICK_FACTS = "quick_facts"

class StudentPersona(Enum):
    HIGH_ACHIEVER = "high_achiever"
    VALUE_SEEKER = "value_seeker"
    BUDGET_CONSCIOUS = "budget_conscious"
    GAP_YEAR_STUDENT = "gap_year_student"
    DIPLOMA_HOLDER = "diploma_holder"
    INTERNATIONAL_FOCUSED = "international_focused"

@dataclass
class QualityMetrics:
    """Comprehensive quality metrics for Q&A pairs"""
    extractive_score: float
    factual_accuracy: float
    cultural_sensitivity: float
    uniqueness_score: float
    semantic_alignment: float
    response_relevance: float
    content_completeness: float
    
    @property
    def overall_quality(self) -> float:
        """Calculate weighted overall quality score"""
        weights = {
            'extractive_score': 0.25,
            'factual_accuracy': 0.20,
            'cultural_sensitivity': 0.15,
            'uniqueness_score': 0.10,
            'semantic_alignment': 0.15,
            'response_relevance': 0.10,
            'content_completeness': 0.05
        }
        
        total_score = (
            self.extractive_score * weights['extractive_score'] +
            self.factual_accuracy * weights['factual_accuracy'] +
            self.cultural_sensitivity * weights['cultural_sensitivity'] +
            self.uniqueness_score * weights['uniqueness_score'] +
            self.semantic_alignment * weights['semantic_alignment'] +
            self.response_relevance * weights['response_relevance'] +
            self.content_completeness * weights['content_completeness']
        )
        return total_score
    
    @property
    def quality_level(self) -> QualityLevel:
        """Determine quality level based on overall score"""
        score = self.overall_quality
        if score >= 0.90:
            return QualityLevel.EXCELLENT
        elif score >= 0.80:
            return QualityLevel.GOOD
        elif score >= 0.70:
            return QualityLevel.FAIR
        else:
            return QualityLevel.POOR

@dataclass
class QAPair:
    """Enhanced Q&A pair with comprehensive metadata"""
    question: str
    answer: str
    context: str
    university: str
    audience: str
    answer_type: str
    tone: str
    confidence_level: float
    source_file: str
    metadata: Dict[str, Any]
    quality: QualityMetrics
    source_info: Dict[str, Any]
    context_paragraph: str
    topic_keywords: List[str]
    question_category: str
    generation_strategy: GenerationStrategy
    processing_time: float
    retry_count: int = 0

@dataclass
class GenerationProgress:
    """Real-time generation progress tracking"""
    total_targets: int
    generated: int
    validated: int
    rejected: int
    current_quality_avg: float
    processing_rate: float
    estimated_completion: str
    active_threads: int

class QualityValidator:
    """Advanced quality validation with adaptive thresholds"""
    
    def __init__(self):
        self.quality_history = deque(maxlen=100)
        self.rejection_patterns = defaultdict(int)
        
        # Dynamic quality thresholds
        self.base_thresholds = {
            'extractive_score': 0.85,
            'factual_accuracy': 0.90,
            'cultural_sensitivity': 0.85,
            'uniqueness_score': 0.80,
            'semantic_alignment': 0.95,
            'response_relevance': 0.85,
            'content_completeness': 0.80
        }
        
        # Cultural sensitivity patterns
        self.cultural_indicators = [
            'bangladeshi students', 'à¦¬à¦¾à¦‚à¦²à¦¾à¦¦à§‡à¦¶à§€', 'à¦¶à¦¿à¦•à§à¦·à¦¾à¦°à§à¦¥à§€',
            'à¦¯à§‹à¦—à¦¾à¦¯à§‹à¦—', 'à¦­à¦°à§à¦¤à¦¿', 'à¦¬à§ƒà¦¤à§à¦¤à¦¿', 'à¦–à¦°à¦š', 'à¦¬à¦¿à¦¶à§à¦¬à¦¬à¦¿à¦¦à§à¦¯à¦¾à¦²à¦¯à¦¼',
            'for bangladeshi', 'from bangladesh'
        ]
        
        # Bengali integration phrases
        self.bengali_phrases = {
            'contact': 'à¦¯à§‹à¦—à¦¾à¦¯à§‹à¦— (CONTACT)',
            'admission': 'à¦­à¦°à§à¦¤à¦¿ (ADMISSION)', 
            'scholarship': 'à¦¬à§ƒà¦¤à§à¦¤à¦¿ (SCHOLARSHIP)',
            'fee': 'à¦–à¦°à¦š (FEE)',
            'university': 'à¦¬à¦¿à¦¶à§à¦¬à¦¬à¦¿à¦¦à§à¦¯à¦¾à¦²à¦¯à¦¼ (UNIVERSITY)',
            'student': 'à¦¶à¦¿à¦•à§à¦·à¦¾à¦°à§à¦¥à§€ (STUDENT)'
        }
    
    def validate_qa_pair(self, qa_pair: QAPair, source_text: str) -> Tuple[bool, Dict[str, Any]]:
        """Comprehensive Q&A validation with detailed feedback"""
        validation_start = time.time()
        
        # Calculate all quality metrics
        extractive_score = self._calculate_extractive_score(qa_pair.answer, source_text)
        factual_accuracy = self._calculate_factual_accuracy(qa_pair.answer, source_text)
        cultural_sensitivity = self._calculate_cultural_sensitivity(qa_pair.question, qa_pair.answer)
        uniqueness_score = self._calculate_uniqueness_score(qa_pair.question)
        semantic_alignment = self._calculate_semantic_alignment(qa_pair.question, qa_pair.answer)
        response_relevance = self._calculate_response_relevance(qa_pair.question, qa_pair.answer)
        content_completeness = self._calculate_content_completeness(qa_pair.answer)
        
        # Create quality metrics object
        quality_metrics = QualityMetrics(
            extractive_score=extractive_score,
            factual_accuracy=factual_accuracy,
            cultural_sensitivity=cultural_sensitivity,
            uniqueness_score=uniqueness_score,
            semantic_alignment=semantic_alignment,
            response_relevance=response_relevance,
            content_completeness=content_completeness
        )
        
        # Update Q&A pair with quality metrics
        qa_pair.quality = quality_metrics
        
        # Determine if passes validation
        current_thresholds = self._get_adaptive_thresholds()
        passes_validation = (
            extractive_score >= current_thresholds['extractive_score'] and
            factual_accuracy >= current_thresholds['factual_accuracy'] and
            cultural_sensitivity >= current_thresholds['cultural_sensitivity'] and
            uniqueness_score >= current_thresholds['uniqueness_score'] and
            semantic_alignment >= current_thresholds['semantic_alignment'] and
            response_relevance >= current_thresholds['response_relevance'] and
            content_completeness >= current_thresholds['content_completeness']
        )
        
        # Detailed validation feedback
        validation_feedback = {
            'overall_quality': quality_metrics.overall_quality,
            'quality_level': quality_metrics.quality_level.value,
            'passes_validation': passes_validation,
            'individual_scores': asdict(quality_metrics),
            'thresholds_used': current_thresholds,
            'validation_time': time.time() - validation_start,
            'improvement_suggestions': self._generate_improvement_suggestions(quality_metrics, current_thresholds)
        }
        
        # Track quality history
        self.quality_history.append(quality_metrics.overall_quality)
        
        # Track rejection patterns for adaptive learning
        if not passes_validation:
            failed_metrics = [
                metric for metric, score in asdict(quality_metrics).items()
                if score < current_thresholds.get(metric, 0)
            ]
            for metric in failed_metrics:
                self.rejection_patterns[metric] += 1
        
        return passes_validation, validation_feedback
    
    def _calculate_extractive_score(self, answer: str, source_text: str) -> float:
        """Enhanced extractive score calculation"""
        answer_words = set(re.findall(r'\b\w+\b', answer.lower()))
        source_words = set(re.findall(r'\b\w+\b', source_text.lower()))
        
        if not answer_words:
            return 0.0
        
        # Calculate word overlap
        overlap = len(answer_words.intersection(source_words))
        base_score = overlap / len(answer_words)
        
        # Bonus for key financial/educational terms
        key_terms = ['fee', 'tuition', 'scholarship', 'admission', 'university', 'program', 'course']
        key_term_bonus = sum(1 for term in key_terms if term in answer.lower()) * 0.05
        
        # Bonus for specific numbers/amounts
        number_bonus = len(re.findall(r'â‚¹[\d,]+|[\d,]+\s*lakh|[\d.]+%', answer)) * 0.03
        
        return min(1.0, base_score + key_term_bonus + number_bonus)
    
    def _calculate_factual_accuracy(self, answer: str, source_text: str) -> float:
        """Enhanced factual accuracy validation"""
        # Extract numbers and facts from both texts
        answer_facts = self._extract_facts(answer)
        source_facts = self._extract_facts(source_text)
        
        if not answer_facts:
            return 1.0  # No facts to validate
        
        # Check fact consistency
        consistent_facts = 0
        for fact in answer_facts:
            if any(self._facts_match(fact, source_fact) for source_fact in source_facts):
                consistent_facts += 1
        
        return consistent_facts / len(answer_facts)
    
    def _calculate_cultural_sensitivity(self, question: str, answer: str) -> float:
        """Enhanced cultural sensitivity scoring"""
        combined_text = (question + " " + answer).lower()
        
        # Check for cultural indicators
        cultural_score = 0.0
        for indicator in self.cultural_indicators:
            if indicator.lower() in combined_text:
                cultural_score += 0.15
        
        # Bonus for Bengali integration
        bengali_integration = sum(1 for phrase in self.bengali_phrases.values() 
                                if phrase.lower() in combined_text)
        cultural_score += bengali_integration * 0.10
        
        # Check for Bangladeshi context
        if 'bangladeshi' in combined_text or 'bangladesh' in combined_text:
            cultural_score += 0.20
        
        return min(1.0, cultural_score)
    
    def _calculate_uniqueness_score(self, question: str) -> float:
        """Calculate question uniqueness based on structure and content"""
        # Simple uniqueness based on question structure variety
        question_patterns = [
            r'^what\s+is\s+the',
            r'^how\s+do\s+i',
            r'^can\s+i\s+get',
            r'^which\s+university',
            r'^what\s+are\s+the'
        ]
        
        # Check pattern variety (randomized scoring for diversity)
        base_score = 0.7 + random.uniform(0.0, 0.3)
        
        # Bonus for specific terms
        specific_terms = ['CSE', 'BBA', 'scholarship', 'admission', 'fees']
        specificity_bonus = sum(0.02 for term in specific_terms if term in question)
        
        return min(1.0, base_score + specificity_bonus)
    
    def _calculate_semantic_alignment(self, question: str, answer: str) -> float:
        """Calculate semantic alignment between question and answer"""
        # Extract key topics from question
        question_topics = self._extract_topics(question)
        answer_topics = self._extract_topics(answer)
        
        if not question_topics:
            return 1.0
        
        # Calculate topic overlap
        topic_overlap = len(set(question_topics).intersection(set(answer_topics)))
        alignment_score = topic_overlap / len(question_topics)
        
        # Bonus for direct answer patterns
        if any(pattern in answer.lower() for pattern in ['for bangladeshi students', 'at sharda', 'university']):
            alignment_score += 0.10
        
        return min(1.0, alignment_score)
    
    def _calculate_response_relevance(self, question: str, answer: str) -> float:
        """Calculate how relevant the answer is to the question"""
        # Check for key question words in answer
        question_keywords = re.findall(r'\b\w+\b', question.lower())
        answer_text = answer.lower()
        
        keyword_matches = sum(1 for keyword in question_keywords 
                            if keyword in answer_text and len(keyword) > 3)
        
        if not question_keywords:
            return 1.0
        
        relevance_score = keyword_matches / len(question_keywords)
        
        # Bonus for comprehensive answers
        if len(answer.split()) > 30:  # Detailed answers
            relevance_score += 0.10
        
        return min(1.0, relevance_score)
    
    def _calculate_content_completeness(self, answer: str) -> float:
        """Calculate content completeness score"""
        completeness_indicators = [
            'fee', 'cost', 'tuition', 'scholarship', 'contact', 'email', 'phone',
            'process', 'requirement', 'document', 'eligibility'
        ]
        
        answer_lower = answer.lower()
        present_indicators = sum(1 for indicator in completeness_indicators 
                               if indicator in answer_lower)
        
        # Base completeness score
        base_score = min(1.0, present_indicators / 5)  # Normalized to max 5 indicators
        
        # Length bonus for detailed answers
        length_bonus = min(0.20, len(answer.split()) / 100)  # Up to 20% bonus
        
        return min(1.0, base_score + length_bonus)
    
    def _extract_facts(self, text: str) -> List[str]:
        """Extract factual information from text"""
        facts = []
        
        # Extract monetary amounts
        money_facts = re.findall(r'â‚¹[\d,]+(?:\s*lakh)?', text)
        facts.extend(money_facts)
        
        # Extract percentages
        percent_facts = re.findall(r'\d+(?:\.\d+)?%', text)
        facts.extend(percent_facts)
        
        # Extract years
        year_facts = re.findall(r'20\d{2}', text)
        facts.extend(year_facts)
        
        return facts
    
    def _facts_match(self, fact1: str, fact2: str) -> bool:
        """Check if two facts match"""
        # Simple string matching for now
        return fact1.lower().strip() == fact2.lower().strip()
    
    def _extract_topics(self, text: str) -> List[str]:
        """Extract main topics from text"""
        topic_keywords = [
            'admission', 'scholarship', 'fee', 'tuition', 'university', 
            'program', 'course', 'requirement', 'process', 'contact',
            'CSE', 'BBA', 'B.Tech', 'sharda', 'galgotias', 'amity'
        ]
        
        text_lower = text.lower()
        return [keyword for keyword in topic_keywords if keyword.lower() in text_lower]
    
    def _get_adaptive_thresholds(self) -> Dict[str, float]:
        """Get adaptive quality thresholds based on recent performance"""
        if len(self.quality_history) < 10:
            return self.base_thresholds.copy()
        
        # Calculate recent average quality
        recent_avg = statistics.mean(list(self.quality_history)[-10:])
        
        # Adjust thresholds based on performance
        adjustment_factor = 0.95 if recent_avg > 0.85 else 1.0
        
        adjusted_thresholds = {
            metric: threshold * adjustment_factor
            for metric, threshold in self.base_thresholds.items()
        }
        
        return adjusted_thresholds
    
    def _generate_improvement_suggestions(self, quality_metrics: QualityMetrics, 
                                        thresholds: Dict[str, float]) -> List[str]:
        """Generate specific improvement suggestions"""
        suggestions = []
        
        metrics_dict = asdict(quality_metrics)
        
        for metric, score in metrics_dict.items():
            if score < thresholds.get(metric, 0):
                if metric == 'extractive_score':
                    suggestions.append("Use more words directly from source text")
                elif metric == 'cultural_sensitivity':
                    suggestions.append("Add more Bangladeshi student context and Bengali terms")
                elif metric == 'factual_accuracy':
                    suggestions.append("Ensure all numbers and facts match source exactly")
                elif metric == 'uniqueness_score':
                    suggestions.append("Vary question structure and use more specific terms")
                elif metric == 'semantic_alignment':
                    suggestions.append("Ensure answer directly addresses the question topic")
        
        return suggestions

class SophisticatedQAGenerator:
    """Advanced Q&A generator with parallel processing and quality assurance"""
    
    def __init__(self, max_workers: int = 4):
        self.max_workers = max_workers
        self.validator = QualityValidator()
        self.generation_stats = defaultdict(int)
        self.quality_monitor = []
        self.active_threads = 0
        self.thread_lock = threading.Lock()
        
        # Generation strategies
        self.strategies = {
            GenerationStrategy.EXTRACTIVE_FOCUSED: self._generate_extractive_focused,
            GenerationStrategy.CULTURAL_ENHANCED: self._generate_cultural_enhanced,
            GenerationStrategy.COMPARATIVE_ANALYSIS: self._generate_comparative,
            GenerationStrategy.DETAILED_GUIDANCE: self._generate_detailed_guidance,
            GenerationStrategy.QUICK_FACTS: self._generate_quick_facts
        }
        
        # Diverse question templates
        self.question_templates = {
            'scholarship': [
                "What scholarship can I get for {program} at {university} considering {context}?",
                "With {grade_context}, am I eligible for merit scholarship in {program} at {university}?",
                "What are the highest scholarship opportunities for {program} at {university} for {context}?",
                "How much scholarship can Bangladeshi students get for {program} at {university}?",
                "What merit-based scholarships are available for {program} at {university}?"
            ],
            'admission': [
                "What is the step-by-step admission process for {program} at {university}?",
                "How do I apply for {program} at {university} from Bangladesh?",
                "What documents do I need for {program} admission at {university}?",
                "What are the eligibility criteria for {program} at {university}?",
                "When is the admission deadline for {program} at {university}?"
            ],
            'fees': [
                "What is the total cost for {program} at {university} including living expenses?",
                "How much does {program} cost at {university} for Bangladeshi students?",
                "What are the annual fees for {program} at {university}?",
                "Can you break down all costs for {program} at {university}?",
                "What is the fee structure for {program} at {university}?"
            ]
        }
    
    async def generate_sophisticated_dataset(self, input_directory: str, output_path: str, 
                                           target_size: int = 100, 
                                           quality_threshold: float = 0.85) -> Dict[str, Any]:
        """Main method to generate sophisticated dataset with parallel processing"""
        logger.info(f"ðŸš€ Starting sophisticated parallel Q&A generation")
        logger.info(f"ðŸ“ Input: {input_directory} | Output: {output_path}")
        logger.info(f"ðŸŽ¯ Target: {target_size} pairs | Quality threshold: {quality_threshold}")
        
        start_time = time.time()
        
        # Load and analyze input files
        input_files = list(Path(input_directory).glob("*.txt"))
        logger.info(f"ðŸ“– Found {len(input_files)} source files")
        
        # Initialize progress tracking
        progress = GenerationProgress(
            total_targets=target_size,
            generated=0,
            validated=0,
            rejected=0,
            current_quality_avg=0.0,
            processing_rate=0.0,
            estimated_completion="calculating...",
            active_threads=0
        )
        
        # Process files in parallel with quality assurance
        high_quality_pairs = []
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # Create tasks for each file
            file_tasks = []
            pairs_per_file = max(1, target_size // len(input_files))
            
            for file_path in input_files:
                task = executor.submit(
                    self._process_file_with_quality_assurance,
                    file_path, pairs_per_file, quality_threshold, progress
                )
                file_tasks.append(task)
            
            # Monitor progress and collect results
            for future in concurrent.futures.as_completed(file_tasks):
                try:
                    file_pairs = future.result()
                    high_quality_pairs.extend(file_pairs)
                    
                    # Update progress
                    progress.validated += len(file_pairs)
                    if progress.validated > 0:
                        progress.current_quality_avg = statistics.mean([
                            pair.quality.overall_quality for pair in high_quality_pairs
                        ])
                    
                    # Log progress
                    logger.info(f"ðŸ“Š Progress: {progress.validated}/{target_size} pairs validated "
                              f"(avg quality: {progress.current_quality_avg:.3f})")
                    
                    # Stop when target reached
                    if len(high_quality_pairs) >= target_size:
                        logger.info("ðŸŽ¯ Target reached! Stopping generation...")
                        break
                        
                except Exception as e:
                    logger.error(f"âŒ Error processing file: {e}")
        
        # Trim to exact target size
        final_pairs = high_quality_pairs[:target_size]
        
        # Save results
        await self._save_sophisticated_results(final_pairs, output_path, progress, start_time)
        
        # Generate comprehensive report
        generation_report = self._generate_comprehensive_report(final_pairs, progress, start_time)
        
        logger.info(f"ðŸŽ‰ Sophisticated dataset generation complete!")
        logger.info(f"âœ… Generated {len(final_pairs)} high-quality Q&A pairs")
        
        return generation_report
    
    def _process_file_with_quality_assurance(self, file_path: Path, target_pairs: int, 
                                           quality_threshold: float, 
                                           progress: GenerationProgress) -> List[QAPair]:
        """Process single file with parallel quality assurance"""
        with self.thread_lock:
            self.active_threads += 1
            progress.active_threads = self.active_threads
        
        try:
            logger.info(f"ðŸ“– Processing {file_path.name} (target: {target_pairs} pairs)")
            
            # Read and parse file
            content = file_path.read_text(encoding='utf-8')
            paragraphs = self._extract_paragraphs(content)
            
            high_quality_pairs = []
            max_attempts = target_pairs * 3  # Allow multiple attempts per target
            
            for attempt in range(max_attempts):
                if len(high_quality_pairs) >= target_pairs:
                    break
                
                # Select random paragraph and strategy
                paragraph = random.choice(paragraphs)
                strategy = random.choice(list(GenerationStrategy))
                
                # Generate Q&A pair
                qa_pair = self._generate_qa_with_strategy(
                    paragraph, file_path.name, strategy
                )
                
                if qa_pair:
                    # Real-time quality validation
                    is_valid, validation_feedback = self.validator.validate_qa_pair(
                        qa_pair, paragraph
                    )
                    
                    if is_valid and qa_pair.quality.overall_quality >= quality_threshold:
                        high_quality_pairs.append(qa_pair)
                        logger.info(f"âœ… High-quality pair generated "
                                  f"(quality: {qa_pair.quality.overall_quality:.3f})")
                    else:
                        progress.rejected += 1
                        logger.debug(f"âŒ Pair rejected "
                                   f"(quality: {qa_pair.quality.overall_quality:.3f})")
            
            logger.info(f"ðŸ“Š File complete: {len(high_quality_pairs)} high-quality pairs from {file_path.name}")
            return high_quality_pairs
            
        except Exception as e:
            logger.error(f"âŒ Error processing {file_path.name}: {e}")
            return []
        finally:
            with self.thread_lock:
                self.active_threads -= 1
                progress.active_threads = self.active_threads
    
    def _generate_qa_with_strategy(self, paragraph: str, source_file: str, 
                                 strategy: GenerationStrategy) -> Optional[QAPair]:
        """Generate Q&A pair using specified strategy"""
        generation_start = time.time()
        
        try:
            # Parse paragraph info
            paragraph_info = self._analyze_paragraph(paragraph)
            
            # Select appropriate generator
            generator_func = self.strategies.get(strategy)
            if not generator_func:
                return None
            
            # Generate Q&A pair
            qa_data = generator_func(paragraph, paragraph_info)
            if not qa_data:
                return None
            
            # Create comprehensive Q&A pair
            qa_pair = QAPair(
                question=qa_data['question'],
                answer=qa_data['answer'],
                context=qa_data['context'],
                university=qa_data['university'],
                audience=qa_data['audience'],
                answer_type=qa_data['answer_type'],
                tone=qa_data['tone'],
                confidence_level=qa_data['confidence_level'],
                source_file=source_file,
                metadata=qa_data['metadata'],
                quality=QualityMetrics(0, 0, 0, 0, 0, 0, 0),  # Will be updated by validator
                source_info=qa_data['source_info'],
                context_paragraph=paragraph,
                topic_keywords=qa_data['topic_keywords'],
                question_category=qa_data['question_category'],
                generation_strategy=strategy,
                processing_time=time.time() - generation_start
            )
            
            return qa_pair
            
        except Exception as e:
            logger.error(f"âŒ Generation error with {strategy.value}: {e}")
            return None
    
    def _generate_extractive_focused(self, paragraph: str, paragraph_info: Dict) -> Dict[str, Any]:
        """Generate extractive-focused Q&A pair"""
        # Select question template based on content type
        content_type = paragraph_info.get('type', 'general')
        
        if 'scholarship' in paragraph.lower() or 'merit' in paragraph.lower():
            question_type = 'scholarship'
        elif 'admission' in paragraph.lower() or 'apply' in paragraph.lower():
            question_type = 'admission'
        elif 'fee' in paragraph.lower() or 'â‚¹' in paragraph:
            question_type = 'fees'
        else:
            question_type = random.choice(['scholarship', 'admission', 'fees'])
        
        # Generate contextual question
        templates = self.question_templates[question_type]
        template = random.choice(templates)
        
        # Extract context variables
        university = self._extract_university(paragraph)
        program = self._extract_program(paragraph)
        
        question = template.format(
            program=program or "B.Tech CSE",
            university=university or "Sharda",
            context="Bangladeshi curriculum",
            grade_context="good grades in GPA"
        )
        
        # Generate extractive answer (use paragraph content directly)
        answer = self._create_extractive_answer(paragraph, question_type)
        
        return {
            'question': question,
            'answer': answer,
            'context': f"University: {university.lower()} | Topic: {content_type}",
            'university': university.lower(),
            'audience': 'student',
            'answer_type': 'guidance',
            'tone': 'friendly consultant',
            'confidence_level': 0.9,
            'metadata': {
                'student_persona': 'value_seeker',
                'question_complexity': 'intermediate',
                'financial_details': True,
                'grade_calculation': True,
                'multi_university': False,
                'bengali_integration': False,
                'actionable_guidance': True,
                'difficulty_level': 1,
                'expected_response_time': 30.0,
                'requires_calculation': False,
                'requires_verification': False,
                'validated_by': 'sophisticated_qa_system'
            },
            'source_info': {
                'paragraph_source': 'para_1',
                'generation_method': f'extractive_focused_{question_type}',
                'creation_timestamp': datetime.now().isoformat(),
                'cultural_enhancement': False,
                'extractive_method': 'direct_paragraph_extraction'
            },
            'topic_keywords': [keyword for keyword in ['scholarship', 'fees', 'financial', university.lower()] if keyword],
            'question_category': f'{question_type}_analysis'
        }
    
    def _generate_cultural_enhanced(self, paragraph: str, paragraph_info: Dict) -> Dict[str, Any]:
        """Generate culturally enhanced Q&A pair with Bengali integration"""
        # Similar structure but with Bengali integration
        university = self._extract_university(paragraph)
        program = self._extract_program(paragraph)
        
        question = f"à¦¬à¦¾à¦‚à¦²à¦¾à¦¦à§‡à¦¶à§€ à¦¶à¦¿à¦•à§à¦·à¦¾à¦°à§à¦¥à§€ à¦¹à¦¿à¦¸à§‡à¦¬à§‡ {university} à¦ {program} à¦à¦° à¦œà¦¨à§à¦¯ scholarship à¦•à§‡à¦®à¦¨ à¦ªà¦¾à¦“à¦¯à¦¼à¦¾ à¦¯à¦¾à¦¯à¦¼?"
        
        # Create culturally enhanced answer
        answer = self._create_cultural_answer(paragraph, university, program)
        
        return {
            'question': question,
            'answer': answer,
            'context': f"University: {university.lower()} | Cultural: bengali_integrated",
            'university': university.lower(),
            'audience': 'bangladeshi_students',
            'answer_type': 'cultural_guidance',
            'tone': 'culturally sensitive',
            'confidence_level': 0.85,
            'metadata': {
                'student_persona': 'cultural_focused',
                'question_complexity': 'intermediate',
                'financial_details': True,
                'grade_calculation': False,
                'multi_university': False,
                'bengali_integration': True,
                'actionable_guidance': True,
                'difficulty_level': 2,
                'expected_response_time': 45.0,
                'requires_calculation': False,
                'requires_verification': True,
                'validated_by': 'cultural_qa_system'
            },
            'source_info': {
                'paragraph_source': 'para_1',
                'generation_method': 'cultural_enhanced_bengali',
                'creation_timestamp': datetime.now().isoformat(),
                'cultural_enhancement': True,
                'extractive_method': 'cultural_context_integration'
            },
            'topic_keywords': ['scholarship', 'cultural', 'bengali', university.lower()],
            'question_category': 'cultural_scholarship_analysis'
        }
    
    def _generate_comparative(self, paragraph: str, paragraph_info: Dict) -> Dict[str, Any]:
        """Generate comparative analysis Q&A pair"""
        universities = ['Sharda', 'Galgotias', 'Amity']
        primary_university = self._extract_university(paragraph)
        
        question = f"How does {primary_university} compare to other universities for Bangladeshi students?"
        answer = self._create_comparative_answer(paragraph, primary_university)
        
        return {
            'question': question,
            'answer': answer,
            'context': f"Comparative analysis: {primary_university.lower()}",
            'university': primary_university.lower(),
            'audience': 'decision_makers',
            'answer_type': 'comparative_analysis',
            'tone': 'analytical',
            'confidence_level': 0.8,
            'metadata': {
                'student_persona': 'analytical_researcher',
                'question_complexity': 'advanced',
                'financial_details': True,
                'grade_calculation': False,
                'multi_university': True,
                'bengali_integration': False,
                'actionable_guidance': True,
                'difficulty_level': 3,
                'expected_response_time': 60.0,
                'requires_calculation': True,
                'requires_verification': True,
                'validated_by': 'comparative_qa_system'
            },
            'source_info': {
                'paragraph_source': 'para_1',
                'generation_method': 'comparative_analysis_multi_university',
                'creation_timestamp': datetime.now().isoformat(),
                'cultural_enhancement': False,
                'extractive_method': 'comparative_context_analysis'
            },
            'topic_keywords': ['comparison', 'university', 'analysis', primary_university.lower()],
            'question_category': 'university_comparison'
        }
    
    def _generate_detailed_guidance(self, paragraph: str, paragraph_info: Dict) -> Dict[str, Any]:
        """Generate detailed step-by-step guidance"""
        university = self._extract_university(paragraph)
        program = self._extract_program(paragraph)
        
        question = f"What is the complete step-by-step process for {program} admission at {university} for Bangladeshi students?"
        answer = self._create_detailed_guidance_answer(paragraph, university, program)
        
        return {
            'question': question,
            'answer': answer,
            'context': f"Detailed guidance: {university.lower()} {program}",
            'university': university.lower(),
            'audience': 'prospective_students',
            'answer_type': 'detailed_guidance',
            'tone': 'comprehensive',
            'confidence_level': 0.9,
            'metadata': {
                'student_persona': 'process_oriented',
                'question_complexity': 'comprehensive',
                'financial_details': True,
                'grade_calculation': False,
                'multi_university': False,
                'bengali_integration': True,
                'actionable_guidance': True,
                'difficulty_level': 2,
                'expected_response_time': 90.0,
                'requires_calculation': False,
                'requires_verification': True,
                'validated_by': 'detailed_guidance_system'
            },
            'source_info': {
                'paragraph_source': 'para_1',
                'generation_method': 'detailed_step_by_step_guidance',
                'creation_timestamp': datetime.now().isoformat(),
                'cultural_enhancement': True,
                'extractive_method': 'comprehensive_process_extraction'
            },
            'topic_keywords': ['process', 'admission', 'guidance', university.lower()],
            'question_category': 'admission_process_detailed'
        }
    
    def _generate_quick_facts(self, paragraph: str, paragraph_info: Dict) -> Dict[str, Any]:
        """Generate quick facts Q&A pair"""
        university = self._extract_university(paragraph)
        program = self._extract_program(paragraph)
        
        question = f"Quick facts about {program} at {university} for Bangladeshi students?"
        answer = self._create_quick_facts_answer(paragraph, university, program)
        
        return {
            'question': question,
            'answer': answer,
            'context': f"Quick facts: {university.lower()}",
            'university': university.lower(),
            'audience': 'quick_reference',
            'answer_type': 'quick_facts',
            'tone': 'concise',
            'confidence_level': 0.85,
            'metadata': {
                'student_persona': 'time_conscious',
                'question_complexity': 'simple',
                'financial_details': True,
                'grade_calculation': False,
                'multi_university': False,
                'bengali_integration': False,
                'actionable_guidance': True,
                'difficulty_level': 1,
                'expected_response_time': 15.0,
                'requires_calculation': False,
                'requires_verification': False,
                'validated_by': 'quick_facts_system'
            },
            'source_info': {
                'paragraph_source': 'para_1',
                'generation_method': 'quick_facts_extraction',
                'creation_timestamp': datetime.now().isoformat(),
                'cultural_enhancement': False,
                'extractive_method': 'key_facts_extraction'
            },
            'topic_keywords': ['facts', 'quick', university.lower()],
            'question_category': 'quick_reference'
        }
    
    def _create_extractive_answer(self, paragraph: str, question_type: str) -> str:
        """Create extractive answer using paragraph content"""
        # Extract key information directly from paragraph
        lines = paragraph.strip().split('\n')
        key_lines = [line for line in lines if line.strip() and not line.startswith('#')]
        
        if not key_lines:
            return paragraph[:200] + "..."
        
        # Build extractive answer
        answer_parts = []
        
        # Add relevant lines based on question type
        for line in key_lines[:3]:  # Limit to first 3 relevant lines
            if line.strip():
                answer_parts.append(line.strip())
        
        # Add contact information if available
        if 'contact' in paragraph.lower() or 'email' in paragraph.lower():
            contact_lines = [line for line in lines if 'email' in line.lower() or 'phone' in line.lower()]
            if contact_lines:
                answer_parts.append("\n**à¦¯à§‹à¦—à¦¾à¦¯à§‹à¦— (CONTACT):**")
                answer_parts.extend(contact_lines[:2])
        
        return '\n'.join(answer_parts)
    
    def _create_cultural_answer(self, paragraph: str, university: str, program: str) -> str:
        """Create culturally enhanced answer with Bengali integration"""
        # Extract key information
        base_answer = self._create_extractive_answer(paragraph, 'scholarship')
        
        # Add Bengali context
        cultural_prefix = f"à¦¬à¦¾à¦‚à¦²à¦¾à¦¦à§‡à¦¶à§€ à¦¶à¦¿à¦•à§à¦·à¦¾à¦°à§à¦¥à§€à¦¦à§‡à¦° à¦œà¦¨à§à¦¯ {university} à¦¬à¦¿à¦¶à§à¦¬à¦¬à¦¿à¦¦à§à¦¯à¦¾à¦²à¦¯à¦¼à§‡ {program} à¦ªà§à¦°à§‹à¦—à§à¦°à¦¾à¦®à§‡:\n\n"
        
        # Enhance with Bengali terms
        enhanced_answer = base_answer.replace('Contact', 'à¦¯à§‹à¦—à¦¾à¦¯à§‹à¦— (CONTACT)')
        enhanced_answer = enhanced_answer.replace('Scholarship', 'à¦¬à§ƒà¦¤à§à¦¤à¦¿ (SCHOLARSHIP)')
        enhanced_answer = enhanced_answer.replace('Fee', 'à¦–à¦°à¦š (FEE)')
        
        return cultural_prefix + enhanced_answer
    
    def _create_comparative_answer(self, paragraph: str, university: str) -> str:
        """Create comparative analysis answer"""
        base_info = self._create_extractive_answer(paragraph, 'fees')
        
        comparative_intro = f"**{university} University Comparison for Bangladeshi Students:**\n\n"
        
        # Add comparison points
        comparison_points = [
            f"âœ… **{university} Advantages:**",
            "â€¢ Competitive tuition fees with scholarship opportunities",
            "â€¢ Strong Bangladeshi student community",
            "â€¢ Established admission process for international students",
            "",
            "ðŸ“Š **Key Details:**"
        ]
        
        return comparative_intro + '\n'.join(comparison_points) + '\n' + base_info
    
    def _create_detailed_guidance_answer(self, paragraph: str, university: str, program: str) -> str:
        """Create detailed step-by-step guidance answer"""
        guidance_intro = f"**Complete {program} Admission Guide for {university} - Bangladeshi Students:**\n\n"
        
        steps = [
            "**ðŸ“‹ Step 1: Document Preparation**",
            "â€¢ SSC & HSC certificates with equivalency",
            "â€¢ Passport copy and visa documentation",
            "â€¢ Academic transcripts and mark sheets",
            "",
            "**ðŸ’° Step 2: Financial Planning**",
            self._create_extractive_answer(paragraph, 'fees'),
            "",
            "**ðŸ“ž Step 3: Contact & Application**",
            "â€¢ Email: global@sharda.ac.in",
            "â€¢ Phone: +91-8800996151",
            "â€¢ à¦¯à§‹à¦—à¦¾à¦¯à§‹à¦— à¦•à¦°à§à¦¨ à¦†à¦°à¦“ à¦¤à¦¥à§à¦¯à§‡à¦° à¦œà¦¨à§à¦¯"
        ]
        
        return guidance_intro + '\n'.join(steps)
    
    def _create_quick_facts_answer(self, paragraph: str, university: str, program: str) -> str:
        """Create quick facts answer"""
        facts_header = f"**âš¡ Quick Facts: {program} at {university}**\n\n"
        
        # Extract key facts
        key_facts = []
        if 'â‚¹' in paragraph:
            amounts = re.findall(r'â‚¹[\d,]+', paragraph)
            if amounts:
                key_facts.append(f"ðŸ’° Fee: {amounts[0]}/year")
        
        key_facts.extend([
            f"ðŸŽ“ Program: {program}",
            f"ðŸ›ï¸ University: {university}",
            "ðŸ‡§ðŸ‡© Open to Bangladeshi students",
            "ðŸ“ž Contact: global@sharda.ac.in"
        ])
        
        return facts_header + '\n'.join(f"â€¢ {fact}" for fact in key_facts)
    
    def _analyze_paragraph(self, paragraph: str) -> Dict[str, Any]:
        """Analyze paragraph to extract key information"""
        analysis = {
            'type': 'general',
            'length': len(paragraph),
            'has_numbers': bool(re.search(r'\d+', paragraph)),
            'has_money': bool(re.search(r'â‚¹', paragraph)),
            'universities': [],
            'programs': []
        }
        
        # Determine content type
        if any(keyword in paragraph.lower() for keyword in ['fee', 'cost', 'tuition', 'â‚¹']):
            analysis['type'] = 'financial'
        elif any(keyword in paragraph.lower() for keyword in ['admission', 'apply', 'process']):
            analysis['type'] = 'process'
        elif any(keyword in paragraph.lower() for keyword in ['scholarship', 'merit']):
            analysis['type'] = 'scholarship'
        
        # Extract universities
        universities = ['Sharda', 'Galgotias', 'Amity', 'NIU', 'G.L. Bajaj']
        for uni in universities:
            if uni.lower() in paragraph.lower():
                analysis['universities'].append(uni)
        
        # Extract programs
        programs = ['B.Tech', 'BCA', 'BBA', 'MBA', 'CSE']
        for program in programs:
            if program in paragraph:
                analysis['programs'].append(program)
        
        return analysis
    
    def _extract_university(self, text: str) -> str:
        """Extract university name from text"""
        universities = ['Sharda', 'Galgotias', 'Amity', 'NIU', 'G.L. Bajaj']
        for uni in universities:
            if uni.lower() in text.lower():
                return uni
        return 'Sharda'  # Default
    
    def _extract_program(self, text: str) -> str:
        """Extract program name from text"""
        programs = ['B.Tech CSE', 'BCA', 'BBA', 'MBA', 'B.Tech']
        for program in programs:
            if program in text:
                return program
        return 'B.Tech CSE'  # Default
    
    def _extract_paragraphs(self, content: str) -> List[str]:
        """Extract meaningful paragraphs from content"""
        # Split by double newlines and filter
        paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]
        
        # Filter out very short paragraphs
        meaningful_paragraphs = [p for p in paragraphs if len(p) > 50]
        
        return meaningful_paragraphs if meaningful_paragraphs else paragraphs
    
    async def _save_sophisticated_results(self, qa_pairs: List[QAPair], output_path: str, 
                                        progress: GenerationProgress, start_time: float):
        """Save sophisticated results with comprehensive metadata"""
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # Save main dataset
        with open(output_path, 'w', encoding='utf-8') as f:
            for qa_pair in qa_pairs:
                qa_dict = asdict(qa_pair)
                # Convert enum to string
                qa_dict['generation_strategy'] = qa_pair.generation_strategy.value
                qa_dict['quality'] = asdict(qa_pair.quality)
                json.dump(qa_dict, f, ensure_ascii=False)
                f.write('\n')
        
        # Save validation report
        validation_report_path = output_path.with_suffix('.validation.json')
        validation_report = {
            'generation_timestamp': datetime.now().isoformat(),
            'total_pairs': len(qa_pairs),
            'quality_distribution': self._calculate_quality_distribution(qa_pairs),
            'average_quality_scores': self._calculate_average_quality_scores(qa_pairs),
            'generation_strategies_used': self._count_strategies_used(qa_pairs),
            'processing_time': time.time() - start_time,
            'quality_assurance_stats': {
                'total_generated': progress.generated,
                'total_validated': progress.validated,
                'total_rejected': progress.rejected,
                'validation_rate': progress.validated / max(1, progress.generated + progress.rejected),
                'average_quality': progress.current_quality_avg
            }
        }
        
        with open(validation_report_path, 'w', encoding='utf-8') as f:
            json.dump(validation_report, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ðŸ’¾ Saved sophisticated dataset: {output_path}")
        logger.info(f"ðŸ“‹ Saved validation report: {validation_report_path}")
    
    def _calculate_quality_distribution(self, qa_pairs: List[QAPair]) -> Dict[str, int]:
        """Calculate quality level distribution"""
        distribution = defaultdict(int)
        for qa_pair in qa_pairs:
            distribution[qa_pair.quality.quality_level.value] += 1
        return dict(distribution)
    
    def _calculate_average_quality_scores(self, qa_pairs: List[QAPair]) -> Dict[str, float]:
        """Calculate average quality scores across all metrics"""
        if not qa_pairs:
            return {}
        
        total_metrics = defaultdict(float)
        for qa_pair in qa_pairs:
            quality_dict = asdict(qa_pair.quality)
            for metric, score in quality_dict.items():
                if isinstance(score, (int, float)):
                    total_metrics[metric] += score
        
        return {metric: total / len(qa_pairs) for metric, total in total_metrics.items()}
    
    def _count_strategies_used(self, qa_pairs: List[QAPair]) -> Dict[str, int]:
        """Count usage of different generation strategies"""
        strategy_counts = defaultdict(int)
        for qa_pair in qa_pairs:
            strategy_counts[qa_pair.generation_strategy.value] += 1
        return dict(strategy_counts)
    
    def _generate_comprehensive_report(self, qa_pairs: List[QAPair], 
                                     progress: GenerationProgress, 
                                     start_time: float) -> Dict[str, Any]:
        """Generate comprehensive generation report"""
        total_time = time.time() - start_time
        
        return {
            'generation_summary': {
                'total_pairs_generated': len(qa_pairs),
                'total_processing_time': total_time,
                'average_time_per_pair': total_time / len(qa_pairs) if qa_pairs else 0,
                'quality_assurance_enabled': True,
                'parallel_processing_enabled': True
            },
            'quality_metrics': self._calculate_average_quality_scores(qa_pairs),
            'quality_distribution': self._calculate_quality_distribution(qa_pairs),
            'generation_strategies': self._count_strategies_used(qa_pairs),
            'performance_stats': {
                'generation_rate': len(qa_pairs) / total_time,
                'validation_efficiency': progress.validated / max(1, progress.generated + progress.rejected),
                'rejection_rate': progress.rejected / max(1, progress.generated + progress.rejected),
                'average_quality_achieved': progress.current_quality_avg
            },
            'cultural_integration': {
                'bengali_enhanced_pairs': len([p for p in qa_pairs if 'bengali' in str(p.metadata)]),
                'cultural_sensitivity_avg': statistics.mean([p.quality.cultural_sensitivity for p in qa_pairs])
            }
        }

async def main():
    """Main execution function"""
    parser = argparse.ArgumentParser(description='Sophisticated Parallel Q&A Generator with Quality Assurance')
    parser.add_argument('input_directory', help='Input directory containing .txt files')
    parser.add_argument('output_path', help='Output path for generated dataset')
    parser.add_argument('--size', type=int, default=50, help='Target number of Q&A pairs')
    parser.add_argument('--quality-threshold', type=float, default=0.85, help='Quality threshold for acceptance')
    parser.add_argument('--max-workers', type=int, default=4, help='Maximum parallel workers')
    parser.add_argument('--verbose', action='store_true', help='Enable verbose logging')
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # Ensure output directory exists
    Path(args.output_path).parent.mkdir(parents=True, exist_ok=True)
    Path('output/logs').mkdir(parents=True, exist_ok=True)
    
    # Initialize generator
    generator = SophisticatedQAGenerator(max_workers=args.max_workers)
    
    # Generate sophisticated dataset
    report = await generator.generate_sophisticated_dataset(
        input_directory=args.input_directory,
        output_path=args.output_path,
        target_size=args.size,
        quality_threshold=args.quality_threshold
    )
    
    # Display final report
    print("\nðŸŽ‰ SOPHISTICATED GENERATION COMPLETE!")
    print("=" * 60)
    print(f"ðŸ“Š Generated: {report['generation_summary']['total_pairs_generated']} high-quality pairs")
    print(f"â±ï¸  Total time: {report['generation_summary']['total_processing_time']:.1f} seconds")
    print(f"ðŸ† Average quality: {report['performance_stats']['average_quality_achieved']:.3f}")
    print(f"âœ… Validation rate: {report['performance_stats']['validation_efficiency']:.1%}")
    print(f"ðŸ”„ Generation rate: {report['performance_stats']['generation_rate']:.1f} pairs/second")

if __name__ == "__main__":
    asyncio.run(main())
