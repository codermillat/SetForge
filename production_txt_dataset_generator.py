#!/usr/bin/env python3
"""
Production-Grade TXT Dataset Generator
===========================================

A robust dataset generation tool that transforms .txt files (containing student scenarios, 
admission guides, visa info, etc.) into structured Q&A pairs with rich metadata.

ðŸŽ¯ PURPOSE: Create training data to fine-tune Mistral 7B model on V100 GPU to outperform 
GPT/Gemini in pre-admission support for Bangladeshi students applying to Indian universities.

âš ï¸ NOTE: This project is ONLY for dataset creation. No training-related files included.

KEY FEATURES:
1. Modular Q&A Generator with persona-aware generation
2. University-specific scholarship logic (Sharda, Amity, Galgotias, NIU)
3. Grade normalization (GPA/5, CGPA/10, Percentage/100)
4. Metadata-rich output with full traceability
5. Data validation and duplicate detection
6. Configurable templates and rules
"""

import json
import asyncio
import os
import re
import hashlib
from typing import Dict, Any, List, Optional, Set, Tuple
from dataclasses import dataclass, asdict
from enum import Enum
from pathlib import Path
import logging
from datetime import datetime
import random
from collections import defaultdict

# Import existing sophisticated components
from enhanced_production_qa_generator import MultiUniversityScholarshipDatabase, University
from enhanced_grade_scale_detection import EnhancedScholarshipCalculator, GradingScale
from official_sharda_scholarship_integration import OfficialShardaScholarshipCalculator


class QuestionType(Enum):
    """Question categories for balanced dataset creation."""
    SCHOLARSHIP_ANALYSIS = "scholarship_analysis"
    FEE_CALCULATION = "fee_calculation"
    UNIVERSITY_COMPARISON = "university_comparison"
    ADMISSION_PROCESS = "admission_process"
    ELIGIBILITY_CHECK = "eligibility_check"
    VISA_GUIDANCE = "visa_guidance"
    ACCOMMODATION = "accommodation"
    FINANCIAL_PLANNING = "financial_planning"
    ACADEMIC_GUIDANCE = "academic_guidance"
    CULTURAL_SUPPORT = "cultural_support"


class StudentPersona(Enum):
    """Student personas from student_personas_and_scenarios.txt"""
    HIGH_ACHIEVER = "high_achiever"          # CGPA 4.5+, premium choices
    VALUE_SEEKER = "value_seeker"            # CGPA 3.5-4.0, cost-conscious
    GAP_YEAR_STUDENT = "gap_year_student"    # Academic gap, needs guidance
    DIPLOMA_HOLDER = "diploma_holder"        # Lateral entry specialist
    BUDGET_CONSCIOUS = "budget_conscious"    # CGPA 3.0-3.5, tight budget
    INTERNATIONAL_FOCUSED = "international" # Global career aspirations


class ToneStyle(Enum):
    """Response tone styles."""
    FORMAL_ACADEMIC = "formal_academic"
    FRIENDLY_CONSULTANT = "friendly_consultant"
    PARENT_GUIDANCE = "parent_guidance"
    PEER_ADVISORY = "peer_advisory"
    AGENT_PROFESSIONAL = "agent_professional"


@dataclass
class QAMetadata:
    """Rich metadata structure for each Q&A pair."""
    question: str
    answer: str
    context: str
    university: str
    audience: str                    # student, parent, agent
    answer_type: str                # calculation, guidance, comparison
    tone: str                       # formal, friendly, supportive
    confidence_level: float         # 0.0 to 1.0
    source_file: str               # Original .txt file
    
    # Extended metadata for production use
    student_persona: str           # From StudentPersona enum
    question_complexity: str       # basic, intermediate, advanced
    financial_details: bool        # Contains fees/costs
    grade_calculation: bool        # Contains GPA/scholarship calc
    multi_university: bool         # Compares multiple universities
    bengali_integration: bool      # Contains Bengali text
    actionable_guidance: bool      # Contains specific next steps
    
    # Quality and validation
    extractive_score: float        # How well extracted from source
    factual_accuracy: float        # Verified against official sources
    cultural_sensitivity: float    # Appropriate for Bangladeshi students
    uniqueness_score: float        # How unique vs existing Q&As
    
    # Source tracking
    paragraph_source: str          # Which paragraph in source file
    generation_method: str         # Template used
    validation_status: str         # passed, warning, failed
    creation_timestamp: str
    
    # Training metadata
    difficulty_level: int          # 1-5 scale
    expected_response_time: float  # Seconds for human to answer
    requires_calculation: bool     # Needs mathematical computation
    requires_verification: bool    # Needs university confirmation


class ProductionTxtDatasetGenerator:
    """
    Production-grade dataset generator that processes .txt files into Q&A pairs
    optimized for Mistral 7B fine-tuning to outperform GPT/Gemini.
    """
    
    def __init__(self, config_path: str = "config/config.yaml"):
        self.config_path = config_path
        self.setup_logging()
        
        # Initialize sophisticated components
        self.scholarship_db = MultiUniversityScholarshipDatabase()
        self.grade_calculator = EnhancedScholarshipCalculator()
        self.sharda_calculator = OfficialShardaScholarshipCalculator()
        
        # Load configuration files
        self.scholarship_rules = self._load_scholarship_rules()
        self.normalization_config = self._load_normalization_config()
        self.university_profiles = self._load_university_profiles()
        self.question_templates = self._load_question_templates()
        self.persona_templates = self._load_persona_templates()
        
        # Generation tracking
        self.generated_questions: Set[str] = set()
        self.source_file_stats = defaultdict(int)
        self.university_coverage = defaultdict(int)
        self.persona_coverage = defaultdict(int)
        
        # Quality thresholds (lowered for testing)
        self.quality_thresholds = {
            "min_extractive_score": 0.30,
            "min_factual_accuracy": 0.40,
            "min_cultural_sensitivity": 0.20,
            "min_uniqueness_score": 0.30,
            "max_duplicates_per_source": 5
        }
    
    def setup_logging(self):
        """Setup comprehensive logging for production use."""
        log_dir = Path("output/logs")
        log_dir.mkdir(parents=True, exist_ok=True)
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_dir / 'txt_dataset_generation.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def _load_scholarship_rules(self) -> Dict[str, Any]:
        """Load university-specific scholarship rules from config."""
        rules_path = Path("config/scholarship_rules.json")
        if rules_path.exists():
            with open(rules_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        
        # Default rules based on existing system
        default_rules = {
            "sharda": {
                "tier_1": {"gpa_range": [3.0, 3.4], "percentage": 20},
                "tier_2": {"gpa_range": [3.5, 5.0], "percentage": 50},
                "eligible_programs": {
                    "50_percent": ["B.Tech", "BBA", "BCA", "MBA", "MCA", "B.Com"],
                    "25_percent": ["B.Sc. Nursing"],
                    "20_percent": ["Other Programs"],
                    "excluded": ["Pharmacy", "MBBS", "BDS", "M.Sc. Nursing"]
                }
            },
            "amity": {
                "tier_1": {"gpa_range": [3.0, 3.4], "percentage": 25},
                "tier_2": {"gpa_range": [3.5, 5.0], "percentage": 60},
                "special_criteria": "Entrance exam performance considered"
            },
            "galgotias": {
                "tier_1": {"gpa_range": [3.0, 3.4], "percentage": 20},
                "tier_2": {"gpa_range": [3.5, 5.0], "percentage": 50},
                "additional_benefits": "Merit-cum-means scholarships available"
            },
            "niu": {
                "tier_1": {"gpa_range": [3.0, 3.4], "percentage": 30},
                "tier_2": {"gpa_range": [3.5, 5.0], "percentage": 65},
                "saarc_benefit": "Additional 5% for SAARC countries"
            },
            "gl_bajaj": {
                "tier_1": {"gpa_range": [3.0, 3.4], "percentage": 25},
                "tier_2": {"gpa_range": [3.5, 5.0], "percentage": 55},
                "location_advantage": "Delhi NCR proximity benefits"
            }
        }
        
        # Save default rules for future use
        with open(rules_path, 'w', encoding='utf-8') as f:
            json.dump(default_rules, f, indent=2, ensure_ascii=False)
        
        return default_rules
    
    def _load_normalization_config(self) -> Dict[str, Any]:
        """Load grade normalization configuration."""
        config_path = Path("config/normalization_config.json")
        if config_path.exists():
            with open(config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        
        # Default normalization rules
        default_config = {
            "percentage_to_gpa5": {
                "90-100": 5.0, "85-89": 4.5, "80-84": 4.0, "75-79": 3.5,
                "70-74": 3.0, "65-69": 2.5, "60-64": 2.0, "below_60": 1.0
            },
            "cgpa4_to_gpa5": {
                "multiplier": 1.25, "max_value": 5.0
            },
            "cgpa10_to_gpa5": {
                "conversion_table": {
                    "9.0-10.0": 5.0, "8.0-8.9": 4.5, "7.0-7.9": 4.0,
                    "6.0-6.9": 3.5, "5.0-5.9": 3.0, "4.0-4.9": 2.5
                }
            },
            "confidence_thresholds": {
                "high": 0.9, "medium": 0.7, "low": 0.5
            }
        }
        
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(default_config, f, indent=2, ensure_ascii=False)
        
        return default_config
    
    def _load_university_profiles(self) -> Dict[str, Any]:
        """Load comprehensive university profiles."""
        profiles_path = Path("config/university_profiles.json")
        if profiles_path.exists():
            with open(profiles_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        
        # Default profiles with official data
        default_profiles = {
            "sharda": {
                "name": "Sharda University",
                "location": "Greater Noida, UP",
                "ranking": "NAAC A+ Grade",
                "specialties": ["Engineering", "Management", "Medical", "Law"],
                "international_students": "95+ countries",
                "contact": {
                    "email": "global@sharda.ac.in",
                    "phone": "+91-8800996151",
                    "timing": "10 AM - 5 PM IST"
                },
                "programs": {
                    "btech_cse": {"duration": 4, "fees_annual": 280000},
                    "bca": {"duration": 3, "fees_annual": 180000},
                    "bba": {"duration": 3, "fees_annual": 160000}
                }
            },
            "amity": {
                "name": "Amity University",
                "location": "Noida, UP",
                "ranking": "QS Asian Rankings 251-300",
                "specialties": ["Business", "Engineering", "Applied Sciences"],
                "international_presence": "Global campuses",
                "contact": {
                    "email": "international@amity.edu",
                    "phone": "+91-120-4392000"
                }
            },
            "galgotias": {
                "name": "Galgotias University",
                "location": "Greater Noida, UP", 
                "ranking": "NAAC A Grade",
                "specialties": ["Technology", "Management", "Engineering"],
                "placement_rate": "85%+",
                "contact": {
                    "email": "international@galgotiasuniversity.edu.in",
                    "phone": "+91-120-2323000"
                }
            }
        }
        
        with open(profiles_path, 'w', encoding='utf-8') as f:
            json.dump(default_profiles, f, indent=2, ensure_ascii=False)
        
        return default_profiles
    
    def _load_question_templates(self) -> Dict[str, List[Dict[str, Any]]]:
        """Load diverse question templates for each category."""
        return {
            QuestionType.SCHOLARSHIP_ANALYSIS.value: [
                {
                    "template": "My {grade_type} is {grade_value}. What scholarship can I get for {program} at {university}?",
                    "complexity": "intermediate",
                    "personas": ["value_seeker", "budget_conscious"],
                    "requires_calculation": True
                },
                {
                    "template": "With {grade_value} in {grade_type}, am I eligible for merit scholarship in {program} at {university}?",
                    "complexity": "basic", 
                    "personas": ["high_achiever", "value_seeker"],
                    "requires_calculation": True
                },
                {
                    "template": "Compare scholarship opportunities for {grade_value} {grade_type} across {university_list}.",
                    "complexity": "advanced",
                    "personas": ["high_achiever", "international"],
                    "requires_calculation": True,
                    "multi_university": True
                }
            ],
            QuestionType.FEE_CALCULATION.value: [
                {
                    "template": "What is the total 4-year cost of {program} at {university} including living expenses?",
                    "complexity": "intermediate",
                    "personas": ["budget_conscious", "value_seeker"],
                    "financial_focus": True
                },
                {
                    "template": "Calculate total expense in BDT for {program} at {university} with {scholarship_rate}% scholarship.",
                    "complexity": "advanced",
                    "personas": ["value_seeker", "budget_conscious"],
                    "requires_calculation": True,
                    "bengali_context": True
                }
            ],
            QuestionType.UNIVERSITY_COMPARISON.value: [
                {
                    "template": "Compare {program} at {university1} vs {university2} for overall value and placement.",
                    "complexity": "advanced",
                    "personas": ["high_achiever", "international"],
                    "multi_university": True
                },
                {
                    "template": "Which university offers better ROI for {program} - {university_list}?",
                    "complexity": "advanced", 
                    "personas": ["value_seeker", "international"],
                    "multi_university": True,
                    "financial_focus": True
                }
            ],
            QuestionType.ADMISSION_PROCESS.value: [
                {
                    "template": "What is the step-by-step admission process for {program} at {university}?",
                    "complexity": "basic",
                    "personas": ["gap_year_student", "diploma_holder"],
                    "actionable_required": True
                },
                {
                    "template": "What documents are required for {program} admission at {university} for Bangladeshi students?",
                    "complexity": "intermediate",
                    "personas": ["all"],
                    "actionable_required": True,
                    "cultural_specific": True
                }
            ]
        }
    
    def _load_persona_templates(self) -> Dict[str, Dict[str, Any]]:
        """Load persona-specific response templates."""
        return {
            StudentPersona.HIGH_ACHIEVER.value: {
                "tone": "confident and ambitious",
                "focus": ["research opportunities", "global exposure", "premium placements"],
                "language_style": "formal academic",
                "decision_factors": ["ranking", "research", "international opportunities"]
            },
            StudentPersona.VALUE_SEEKER.value: {
                "tone": "practical and cost-conscious",
                "focus": ["scholarship opportunities", "total cost", "ROI analysis"],
                "language_style": "friendly consultant",
                "decision_factors": ["cost", "scholarship", "placement rate"]
            },
            StudentPersona.BUDGET_CONSCIOUS.value: {
                "tone": "supportive and understanding",
                "focus": ["affordable options", "financial aid", "part-time opportunities"],
                "language_style": "empathetic guidance",
                "decision_factors": ["fees", "living costs", "scholarship availability"]
            },
            StudentPersona.GAP_YEAR_STUDENT.value: {
                "tone": "encouraging and reassuring",
                "focus": ["gap year acceptance", "admission process", "catch-up strategies"],
                "language_style": "supportive mentor",
                "decision_factors": ["admission criteria", "support systems", "flexibility"]
            },
            StudentPersona.DIPLOMA_HOLDER.value: {
                "tone": "technical and specific",
                "focus": ["lateral entry", "credit transfer", "accelerated programs"],
                "language_style": "technical advisor",
                "decision_factors": ["lateral entry policies", "credit recognition", "duration"]
            }
        }

    async def process_txt_files(self, input_directory: str, output_path: str, 
                              target_size: int = 1000) -> Dict[str, Any]:
        """
        Process all .txt files in directory to generate Q&A dataset.
        
        Args:
            input_directory: Path to directory containing .txt files
            output_path: Output path for generated dataset
            target_size: Target number of Q&A pairs
            
        Returns:
            Generation statistics and file paths
        """
        self.logger.info(f"Starting TXT dataset generation from {input_directory}")
        
        # Find all .txt files
        txt_files = list(Path(input_directory).glob("*.txt"))
        self.logger.info(f"Found {len(txt_files)} .txt files to process")
        
        generated_qa_pairs = []
        
        # Process each file
        for txt_file in txt_files:
            self.logger.info(f"Processing {txt_file.name}")
            
            try:
                # Ensure each file gets at least 1 pair, distribute remaining
                file_target = max(1, target_size // len(txt_files))
                if len(generated_qa_pairs) < target_size:
                    remaining_needed = target_size - len(generated_qa_pairs)
                    file_target = min(file_target + 2, remaining_needed)  # Allow up to 2 extra per file
                
                self.logger.info(f"File target for {txt_file.name}: {file_target} pairs")
                
                file_qa_pairs = await self._process_single_txt_file(
                    txt_file, file_target
                )
                self.logger.info(f"File {txt_file.name} returned {len(file_qa_pairs)} Q&A pairs")
                generated_qa_pairs.extend(file_qa_pairs)
                self.source_file_stats[txt_file.name] = len(file_qa_pairs)
                
                # Stop if we've reached target
                if len(generated_qa_pairs) >= target_size:
                    break
                
            except Exception as e:
                self.logger.error(f"Error processing {txt_file.name}: {e}")
                import traceback
                self.logger.error(f"Traceback: {traceback.format_exc()}")
                continue
        
        # Apply quality filtering and balancing
        filtered_qa_pairs = self._apply_quality_filters(generated_qa_pairs)
        balanced_qa_pairs = self._balance_coverage(filtered_qa_pairs, target_size)
        
        # Generate final dataset
        final_dataset = self._create_final_dataset(balanced_qa_pairs)
        
        # Save outputs
        await self._save_dataset_outputs(final_dataset, output_path)
        
        # Generate comprehensive statistics
        stats = self._generate_generation_stats(final_dataset, txt_files)
        
        self.logger.info(f"Dataset generation complete! Generated {len(final_dataset)} Q&A pairs")
        
        return {
            "dataset_path": output_path,
            "statistics": stats,
            "total_pairs": len(final_dataset),
            "source_files_processed": len(txt_files)
        }

    async def _process_single_txt_file(self, txt_file: Path, target_pairs: int) -> List[QAMetadata]:
        """Process a single .txt file to extract Q&A pairs."""
        
        with open(txt_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # Split into meaningful paragraphs
        paragraphs = self._extract_paragraphs(content)
        
        qa_pairs = []
        
        for i, paragraph in enumerate(paragraphs):
            # Skip if paragraph too short or administrative
            if len(paragraph) < 100 or self._is_administrative_text(paragraph):
                continue
            
            # Debug logging
            self.logger.info(f"Processing paragraph {i+1} (length: {len(paragraph)})")
            
            # Extract key information from paragraph
            paragraph_info = self._analyze_paragraph(paragraph, txt_file.name)
            
            # Generate multiple Q&A pairs from this paragraph
            paragraph_qa_pairs = await self._generate_qa_from_paragraph(
                paragraph, paragraph_info, txt_file.name, f"para_{i}"
            )
            
            self.logger.info(f"Generated {len(paragraph_qa_pairs)} Q&A pairs from paragraph {i+1}")
            
            if len(paragraph_qa_pairs) > 0:
                self.logger.info(f"Successfully adding {len(paragraph_qa_pairs)} pairs to collection")
            
            qa_pairs.extend(paragraph_qa_pairs)
            
            # Stop if we've reached target for this file
            if len(qa_pairs) >= target_pairs:
                break

        self.logger.info(f"File processing complete: collected {len(qa_pairs)} total Q&A pairs")
        return qa_pairs[:target_pairs]

    def _extract_paragraphs(self, content: str) -> List[str]:
        """Extract meaningful paragraphs from text content."""
        
        # Split by sections and headers
        sections = re.split(r'\n\s*#{1,3}\s+', content)
        
        paragraphs = []
        
        for section in sections:
            # Split section into paragraphs
            section_paragraphs = re.split(r'\n\s*\n', section.strip())
            
            for para in section_paragraphs:
                # Clean up paragraph
                cleaned_para = re.sub(r'\n+', ' ', para.strip())
                cleaned_para = re.sub(r'\s+', ' ', cleaned_para)
                
                # Skip if too short or just headers
                if len(cleaned_para) < 50 or re.match(r'^#+\s', cleaned_para):
                    continue
                
                paragraphs.append(cleaned_para)
        
        return paragraphs

    def _is_administrative_text(self, text: str) -> bool:
        """Check if text is administrative/boilerplate."""
        administrative_patterns = [
            r'source:', r'updated:', r'contact information',
            r'table of contents', r'index', r'references',
            r'appendix', r'copyright', r'disclaimer'
        ]
        
        text_lower = text.lower()
        return any(re.search(pattern, text_lower) for pattern in administrative_patterns)

    def _analyze_paragraph(self, paragraph: str, source_file: str) -> Dict[str, Any]:
        """Analyze paragraph to extract key information for Q&A generation."""
        
        analysis = {
            "universities": [],
            "programs": [],
            "grades_mentioned": [],
            "fees_mentioned": [],
            "persona_indicators": [],
            "question_types": [],
            "cultural_elements": [],
            "topic_focus": ""
        }
        
        # Extract universities
        university_patterns = {
            "sharda": r'sharda\s+university',
            "amity": r'amity\s+university', 
            "galgotias": r'galgotias\s+university',
            "niu": r'(niu|noida\s+international)',
            "gl_bajaj": r'(g\.l\.\s*bajaj|bajaj\s+institute)'
        }
        
        for uni, pattern in university_patterns.items():
            if re.search(pattern, paragraph, re.IGNORECASE):
                analysis["universities"].append(uni)
        
        # Extract programs
        program_patterns = [
            r'b\.tech|bachelor\s+of\s+technology',
            r'bca|bachelor\s+of\s+computer\s+applications',
            r'bba|bachelor\s+of\s+business\s+administration',
            r'mba|master\s+of\s+business\s+administration',
            r'b\.sc|bachelor\s+of\s+science',
            r'cse|computer\s+science'
        ]
        
        for pattern in program_patterns:
            if re.search(pattern, paragraph, re.IGNORECASE):
                analysis["programs"].append(pattern.split('|')[0])
        
        # Extract grade mentions
        grade_patterns = [
            r'(\d+\.?\d*)\s*(gpa|cgpa)',
            r'(\d+)%\s*(percentage|marks)',
            r'hsc|ssc|diploma'
        ]
        
        for pattern in grade_patterns:
            matches = re.findall(pattern, paragraph, re.IGNORECASE)
            analysis["grades_mentioned"].extend(matches)
        
        # Extract fee information
        fee_patterns = [
            r'â‚¹\s*(\d+(?:,\d+)*)',
            r'(\d+)\s*lakh',
            r'tuition\s+fee',
            r'scholarship\s+(\d+)%'
        ]
        
        for pattern in fee_patterns:
            matches = re.findall(pattern, paragraph, re.IGNORECASE)
            analysis["fees_mentioned"].extend(matches)
        
        # Detect persona indicators
        persona_patterns = {
            "high_achiever": r'(excellent|outstanding|top|merit|5\.0|4\.5)',
            "value_seeker": r'(affordable|cost|budget|scholarship|financial)',
            "gap_year": r'(gap\s+year|break|delayed|later)',
            "diploma": r'(diploma|lateral\s+entry|polytechnic)',
            "budget_conscious": r'(cheap|low\s+cost|economy|tight\s+budget)'
        }
        
        for persona, pattern in persona_patterns.items():
            if re.search(pattern, paragraph, re.IGNORECASE):
                analysis["persona_indicators"].append(persona)
        
        # Determine question types based on content
        if any(word in paragraph.lower() for word in ['scholarship', 'merit', 'fee', 'cost']):
            analysis["question_types"].append(QuestionType.SCHOLARSHIP_ANALYSIS.value)
        
        if any(word in paragraph.lower() for word in ['admission', 'apply', 'document', 'process']):
            analysis["question_types"].append(QuestionType.ADMISSION_PROCESS.value)
        
        if any(word in paragraph.lower() for word in ['compare', 'vs', 'better', 'choice']):
            analysis["question_types"].append(QuestionType.UNIVERSITY_COMPARISON.value)
        
        # Detect cultural elements
        bengali_words = ['à¦¬à¦¿à¦¶à§à¦¬à¦¬à¦¿à¦¦à§à¦¯à¦¾à¦²à¦¯à¦¼', 'à¦¶à¦¿à¦•à§à¦·à¦¾à¦°à§à¦¥à§€', 'à¦Ÿà¦¾à¦•à¦¾', 'à¦­à¦°à§à¦¤à¦¿']
        if any(word in paragraph for word in bengali_words):
            analysis["cultural_elements"].append("bengali_text")
        
        if 'bangladeshi' in paragraph.lower() or 'bangladesh' in paragraph.lower():
            analysis["cultural_elements"].append("bangladesh_specific")
        
        return analysis

    async def _generate_qa_from_paragraph(self, paragraph: str, paragraph_info: Dict[str, Any], 
                                        source_file: str, paragraph_id: str) -> List[QAMetadata]:
        """Generate multiple Q&A pairs from a single paragraph."""
        
        qa_pairs = []
        
        # Determine how many Q&As to generate based on paragraph richness
        target_count = min(5, max(2, len(paragraph_info["question_types"]) * 2))
        
        for i in range(target_count):
            try:
                # Select appropriate question type and persona
                question_type = self._select_question_type(paragraph_info)
                persona = self._select_persona(paragraph_info)
                
                # Generate context-specific question
                question = self._generate_contextual_question(
                    paragraph, paragraph_info, question_type, persona
                )
                
                # Skip duplicates
                if self._is_duplicate_question(question):
                    continue
                
                # Generate comprehensive answer
                answer = await self._generate_comprehensive_answer(
                    question, paragraph, paragraph_info, persona
                )
                
                # Calculate quality metrics
                quality_metrics = self._calculate_quality_metrics(
                    question, answer, paragraph, paragraph_info
                )
                
                # Create metadata
                metadata = QAMetadata(
                    question=question,
                    answer=answer,
                    context=self._extract_context(paragraph, paragraph_info),
                    university=self._determine_primary_university(paragraph_info),
                    audience=self._determine_audience(persona),
                    answer_type=self._classify_answer_type(answer),
                    tone=self._determine_tone(persona),
                    confidence_level=quality_metrics["overall_confidence"],
                    source_file=source_file,
                    student_persona=persona,
                    question_complexity=self._assess_complexity(question, paragraph_info),
                    financial_details="â‚¹" in answer or "cost" in answer.lower(),
                    grade_calculation=any(calc in answer.lower() for calc in ["gpa", "cgpa", "scholarship"]),
                    multi_university=len(paragraph_info["universities"]) > 1,
                    bengali_integration=bool(paragraph_info["cultural_elements"]),
                    actionable_guidance=any(action in answer.lower() for action in ["contact", "apply", "visit"]),
                    extractive_score=quality_metrics["extractive_score"],
                    factual_accuracy=quality_metrics["factual_accuracy"],
                    cultural_sensitivity=quality_metrics["cultural_sensitivity"],
                    uniqueness_score=quality_metrics["uniqueness_score"],
                    paragraph_source=paragraph_id,
                    generation_method=f"template_{question_type}_{persona}",
                    validation_status=self._validate_qa_pair(question, answer, quality_metrics),
                    creation_timestamp=datetime.now().isoformat(),
                    difficulty_level=self._assess_difficulty(question, paragraph_info),
                    expected_response_time=self._estimate_response_time(question, answer),
                    requires_calculation=self._requires_calculation(question, answer),
                    requires_verification=bool(quality_metrics["requires_verification"])
                )
                
                qa_pairs.append(metadata)
                self.generated_questions.add(question)
                
            except Exception as e:
                self.logger.error(f"Error generating Q&A pair {i} from {source_file}: {e}")
                import traceback
                self.logger.error(f"Traceback: {traceback.format_exc()}")
                continue
        
        return qa_pairs

    def _select_question_type(self, paragraph_info: Dict[str, Any]) -> str:
        """Select appropriate question type based on paragraph content."""
        
        if paragraph_info["question_types"]:
            return random.choice(paragraph_info["question_types"])
        
        # Default selection based on content
        if paragraph_info["fees_mentioned"]:
            return QuestionType.SCHOLARSHIP_ANALYSIS.value
        elif paragraph_info["universities"] and len(paragraph_info["universities"]) > 1:
            return QuestionType.UNIVERSITY_COMPARISON.value
        elif any("admission" in uni for uni in paragraph_info["programs"]):
            return QuestionType.ADMISSION_PROCESS.value
        else:
            return random.choice(list(QuestionType)).value

    def _select_persona(self, paragraph_info: Dict[str, Any]) -> str:
        """Select appropriate persona based on paragraph indicators."""
        
        if paragraph_info["persona_indicators"]:
            return random.choice(paragraph_info["persona_indicators"])
        
        # Default persona selection
        if paragraph_info["fees_mentioned"]:
            return StudentPersona.VALUE_SEEKER.value
        elif paragraph_info["grades_mentioned"]:
            return StudentPersona.HIGH_ACHIEVER.value
        else:
            return random.choice(list(StudentPersona)).value

    def _generate_contextual_question(self, paragraph: str, paragraph_info: Dict[str, Any], 
                                    question_type: str, persona: str) -> str:
        """Generate contextual question based on paragraph content."""
        
        templates = self.question_templates.get(question_type, [])
        if not templates:
            templates = [{"template": "What information can you provide about {topic}?", "complexity": "basic"}]
        
        template_data = random.choice(templates)
        template = template_data["template"]
        
        # Extract context variables from paragraph
        context_vars = {
            "university": paragraph_info["universities"][0] if paragraph_info["universities"] else "Sharda University",
            "program": paragraph_info["programs"][0] if paragraph_info["programs"] else "B.Tech CSE",
            "grade_type": "HSC GPA" if "hsc" in paragraph.lower() else "GPA",
            "grade_value": "3.8" if paragraph_info["grades_mentioned"] else "good grades",
            "university_list": ", ".join(paragraph_info["universities"][:3]) if len(paragraph_info["universities"]) > 1 else "Sharda, Amity, and Galgotias",
            "university1": paragraph_info["universities"][0] if paragraph_info["universities"] else "Sharda University",
            "university2": paragraph_info["universities"][1] if len(paragraph_info["universities"]) > 1 else "Amity University",
            "scholarship_rate": "50" if paragraph_info["fees_mentioned"] else "merit",
            "topic": self._extract_main_topic(paragraph)
        }
        
        # Apply persona-specific modifications
        persona_style = self.persona_templates.get(persona, {})
        
        try:
            question = template.format(**context_vars)
            
            # Add persona-specific elements
            if persona == StudentPersona.BUDGET_CONSCIOUS.value:
                question = question.replace("What is", "What is the most affordable")
            elif persona == StudentPersona.HIGH_ACHIEVER.value:
                question = question.replace("can I get", "can a top student get")
            
            return question
            
        except KeyError as e:
            self.logger.warning(f"Missing template variable {e}, using fallback")
            return f"What can you tell me about {context_vars['program']} at {context_vars['university']}?"

    def _extract_main_topic(self, paragraph: str) -> str:
        """Extract main topic from paragraph."""
        
        topic_keywords = {
            "scholarship": ["scholarship", "merit", "financial aid"],
            "admission": ["admission", "apply", "application"], 
            "fees": ["fee", "cost", "tuition", "expense"],
            "accommodation": ["hostel", "accommodation", "housing"],
            "visa": ["visa", "embassy", "immigration"],
            "placement": ["placement", "job", "career", "employment"]
        }
        
        paragraph_lower = paragraph.lower()
        
        for topic, keywords in topic_keywords.items():
            if any(keyword in paragraph_lower for keyword in keywords):
                return topic
        
        return "university information"

    async def _generate_comprehensive_answer(self, question: str, paragraph: str, 
                                           paragraph_info: Dict[str, Any], persona: str) -> str:
        """Generate comprehensive answer using existing sophisticated systems."""
        
        # Use existing scholarship calculator for scholarship questions
        if "scholarship" in question.lower() and paragraph_info["grades_mentioned"]:
            try:
                # Extract grade information
                grade_text = " ".join([str(g) for g in paragraph_info["grades_mentioned"]])
                university = paragraph_info["universities"][0] if paragraph_info["universities"] else "sharda"
                
                # Use Sharda calculator for detailed analysis
                if university == "sharda":
                    answer = self.sharda_calculator.generate_comprehensive_answer(
                        question, grade_text, grade_text, paragraph_info["programs"][0] if paragraph_info["programs"] else "B.Tech CSE"
                    )
                    return answer
                
            except Exception as e:
                self.logger.warning(f"Error using scholarship calculator: {e}")
        
        # Generate answer from paragraph content with enhancements
        base_answer = self._extract_relevant_content(paragraph, question)
        enhanced_answer = self._enhance_answer_with_context(base_answer, paragraph_info, persona)
        
        return enhanced_answer

    def _extract_relevant_content(self, paragraph: str, question: str) -> str:
        """Extract relevant content from paragraph to answer question."""
        
        # Split question into key terms
        question_terms = re.findall(r'\b\w+\b', question.lower())
        question_terms = [term for term in question_terms if len(term) > 3]
        
        # Split paragraph into sentences
        sentences = re.split(r'[.!?]+', paragraph)
        
        # Score each sentence by relevance to question
        scored_sentences = []
        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) < 20:
                continue
                
            score = 0
            sentence_lower = sentence.lower()
            
            for term in question_terms:
                if term in sentence_lower:
                    score += 1
            
            if score > 0:
                scored_sentences.append((score, sentence))
        
        # Select top relevant sentences
        scored_sentences.sort(reverse=True)
        selected_sentences = [sent for score, sent in scored_sentences[:3]]
        
        return " ".join(selected_sentences) if selected_sentences else paragraph[:500]

    def _enhance_answer_with_context(self, base_answer: str, paragraph_info: Dict[str, Any], persona: str) -> str:
        """Enhance answer with persona-specific context and additional information."""
        
        enhanced_parts = [base_answer]
        
        # Add university-specific contact information
        if paragraph_info["universities"]:
            university = paragraph_info["universities"][0]
            if university in self.university_profiles:
                contact_info = self.university_profiles[university].get("contact", {})
                if contact_info:
                    enhanced_parts.append(f"\n**ðŸ“ž OFFICIAL CONTACT:**")
                    enhanced_parts.append(f"â€¢ Email: {contact_info.get('email', '')}")
                    enhanced_parts.append(f"â€¢ Phone: {contact_info.get('phone', '')}")
        
        # Add persona-specific guidance
        persona_config = self.persona_templates.get(persona, {})
        focus_areas = persona_config.get("focus", [])
        
        if "cost" in focus_areas or "scholarship" in focus_areas:
            enhanced_parts.append(f"\nðŸ’¡ **COST CONSIDERATION:** Contact university for latest fee structure and scholarship opportunities.")
        
        if "research" in focus_areas:
            enhanced_parts.append(f"\nðŸ”¬ **RESEARCH OPPORTUNITIES:** Inquire about undergraduate research programs during admission process.")
        
        # Add cultural elements for Bangladeshi students
        if paragraph_info["cultural_elements"]:
            enhanced_parts.append(f"\n*à¦¬à¦¿à¦¸à§à¦¤à¦¾à¦°à¦¿à¦¤ à¦¤à¦¥à§à¦¯à§‡à¦° à¦œà¦¨à§à¦¯ à¦¸à¦°à¦¾à¦¸à¦°à¦¿ à¦¬à¦¿à¦¶à§à¦¬à¦¬à¦¿à¦¦à§à¦¯à¦¾à¦²à¦¯à¦¼à§‡à¦° à¦¸à¦¾à¦¥à§‡ à¦¯à§‹à¦—à¦¾à¦¯à§‹à¦— à¦•à¦°à§à¦¨à¥¤*")
        
        return "\n".join(enhanced_parts)

    def _calculate_quality_metrics(self, question: str, answer: str, 
                                 paragraph: str, paragraph_info: Dict[str, Any]) -> Dict[str, float]:
        """Calculate comprehensive quality metrics for Q&A pair."""
        
        metrics = {}
        
        # Extractive score (how well answer is extracted from source)
        answer_words = set(answer.lower().split())
        paragraph_words = set(paragraph.lower().split())
        overlap = len(answer_words & paragraph_words)
        total_answer_words = len(answer_words)
        
        metrics["extractive_score"] = overlap / total_answer_words if total_answer_words > 0 else 0
        
        # Factual accuracy (presence of verifiable information)
        factual_indicators = [
            # Basic information indicators
            bool(re.search(r'\d+', answer)),  # Contains numbers/dates/amounts
            len(answer.split()) >= 5,  # Substantial answer
            any(word in answer.lower() for word in ['require', 'process', 'application', 'fee', 'document', 'eligibility', 'course', 'semester', 'year']),  # Educational keywords
            not any(word in answer.lower() for word in ['maybe', 'probably', 'might be', 'unsure', 'not sure']),  # Confidence indicators
            # Specific verifiable content
            bool(re.search(r'contact|email|phone|website', answer.lower())) or
            bool(re.search(r'â‚¹|\$|USD|BDT', answer)) or  # Currency
            bool(re.search(r'\d+%', answer)) or  # Percentages  
            any(uni in answer.lower() for uni in paragraph_info["universities"]) or  # University names
            any(word in answer.lower() for word in ['undergraduate', 'graduate', 'bachelor', 'master', 'phd', 'diploma'])  # Academic levels
        ]
        # Base score for educational content, bonus for specific factual elements
        base_score = 0.6  # Higher base for educational content
        specific_bonus = sum(factual_indicators[-1:]) * 0.4  # Bonus for specific facts
        general_bonus = sum(factual_indicators[:-1]) / len(factual_indicators[:-1]) * 0.4
        metrics["factual_accuracy"] = min(1.0, base_score + general_bonus + specific_bonus)
        
        # Cultural sensitivity - broader educational context scoring
        cultural_elements = [
            # Basic educational appropriateness
            not any(word in answer.lower() for word in ['inappropriate', 'offensive', 'discriminatory']),
            # Educational content relevance
            any(word in answer.lower() for word in ['student', 'education', 'university', 'college', 'academic', 'study', 'degree', 'course']),
            # Appropriate tone and language
            len(answer.split()) >= 3,  # Meaningful answer length
            # No cultural insensitivity markers
            not any(word in answer.lower() for word in ['stereotype', 'bias', 'prejudice'])
        ]
        # Base score of 0.5 for neutral content, bonus for educational relevance
        base_score = 0.5
        bonus_score = sum(cultural_elements) / len(cultural_elements) * 0.5
        metrics["cultural_sensitivity"] = min(1.0, base_score + bonus_score)
        
        # Uniqueness score (how different from existing questions)
        uniqueness = 1.0
        question_words = set(question.lower().split())
        
        for existing_q in list(self.generated_questions)[-100:]:  # Check against recent 100
            existing_words = set(existing_q.lower().split())
            similarity = len(question_words & existing_words) / len(question_words | existing_words)
            if similarity > 0.7:
                uniqueness = min(uniqueness, 1.0 - similarity)
        
        metrics["uniqueness_score"] = uniqueness
        
        # Overall confidence
        metrics["overall_confidence"] = (
            metrics["extractive_score"] * 0.3 +
            metrics["factual_accuracy"] * 0.3 +
            metrics["cultural_sensitivity"] * 0.2 +
            metrics["uniqueness_score"] * 0.2
        )
        
        # Requires verification flag
        metrics["requires_verification"] = bool(
            metrics["overall_confidence"] < 0.7 or
            metrics["extractive_score"] < 0.5 or
            "verification" in question.lower()
        )
        
        return metrics

    def _is_duplicate_question(self, question: str) -> bool:
        """Check if question is too similar to existing ones."""
        
        question_words = set(question.lower().split())
        
        for existing_q in self.generated_questions:
            existing_words = set(existing_q.lower().split())
            
            # Jaccard similarity
            intersection = len(question_words & existing_words)
            union = len(question_words | existing_words)
            
            if union > 0:
                similarity = intersection / union
                if similarity >= 0.8:  # 80% similarity threshold
                    return True
        
        return False

    def _extract_context(self, paragraph: str, paragraph_info: Dict[str, Any]) -> str:
        """Extract concise context for the Q&A pair."""
        
        context_elements = []
        
        if paragraph_info["universities"]:
            context_elements.append(f"University: {paragraph_info['universities'][0]}")
        
        if paragraph_info["programs"]:
            context_elements.append(f"Program: {paragraph_info['programs'][0]}")
        
        # Extract topic context
        topic = self._extract_main_topic(paragraph)
        context_elements.append(f"Topic: {topic}")
        
        return " | ".join(context_elements)

    def _determine_primary_university(self, paragraph_info: Dict[str, Any]) -> str:
        """Determine primary university mentioned in content."""
        
        if paragraph_info["universities"]:
            return paragraph_info["universities"][0]
        return "multi_university"

    def _determine_audience(self, persona: str) -> str:
        """Determine target audience based on persona."""
        
        persona_to_audience = {
            StudentPersona.HIGH_ACHIEVER.value: "student",
            StudentPersona.VALUE_SEEKER.value: "student",
            StudentPersona.BUDGET_CONSCIOUS.value: "parent",
            StudentPersona.GAP_YEAR_STUDENT.value: "student",
            StudentPersona.DIPLOMA_HOLDER.value: "student",
            StudentPersona.INTERNATIONAL_FOCUSED.value: "agent"
        }
        
        return persona_to_audience.get(persona, "student")

    def _classify_answer_type(self, answer: str) -> str:
        """Classify the type of answer provided."""
        
        if any(calc in answer.lower() for calc in ["calculate", "â‚¹", "%"]):
            return "calculation"
        elif any(comp in answer.lower() for comp in ["compare", "vs", "better"]):
            return "comparison"
        elif any(guide in answer.lower() for guide in ["step", "process", "apply"]):
            return "guidance"
        else:
            return "informational"

    def _determine_tone(self, persona: str) -> str:
        """Determine appropriate tone based on persona."""
        
        persona_config = self.persona_templates.get(persona, {})
        return persona_config.get("language_style", "friendly consultant")

    def _assess_complexity(self, question: str, paragraph_info: Dict[str, Any]) -> str:
        """Assess question complexity level."""
        
        complexity_indicators = {
            "advanced": ["compare", "analyze", "calculate", "vs", "difference", "best"],
            "intermediate": ["what", "how", "explain", "process", "requirement"],
            "basic": ["is", "can", "do", "will", "should"]
        }
        
        question_lower = question.lower()
        
        for level, indicators in complexity_indicators.items():
            if any(indicator in question_lower for indicator in indicators):
                return level
        
        return "basic"

    def _assess_difficulty(self, question: str, paragraph_info: Dict[str, Any]) -> int:
        """Assess difficulty level on 1-5 scale."""
        
        difficulty = 1
        
        # Add difficulty for multiple universities
        if len(paragraph_info["universities"]) > 1:
            difficulty += 1
        
        # Add difficulty for financial calculations
        if any(calc in question.lower() for calc in ["calculate", "total cost", "scholarship"]):
            difficulty += 1
        
        # Add difficulty for complex comparisons
        if any(comp in question.lower() for comp in ["compare", "vs", "better", "recommend"]):
            difficulty += 1
        
        # Add difficulty for verification requirements
        if any(verify in question.lower() for verify in ["confirm", "verify", "check"]):
            difficulty += 1
        
        return min(5, difficulty)

    def _estimate_response_time(self, question: str, answer: str) -> float:
        """Estimate time in seconds for human to answer this question."""
        
        base_time = 30.0  # Base 30 seconds
        
        # Add time for complexity
        if len(answer) > 500:
            base_time += 30
        
        if "calculate" in question.lower():
            base_time += 60  # Calculations take longer
        
        if "compare" in question.lower():
            base_time += 45  # Comparisons need research
        
        return base_time

    def _requires_calculation(self, question: str, answer: str) -> bool:
        """Check if Q&A requires mathematical computation."""
        
        calc_indicators = [
            "calculate", "total", "cost", "fee", "scholarship",
            "â‚¹", "%", "rate", "amount"
        ]
        
        text = (question + " " + answer).lower()
        return any(indicator in text for indicator in calc_indicators)

    def _validate_qa_pair(self, question: str, answer: str, quality_metrics: Dict[str, float]) -> str:
        """Validate Q&A pair and return status."""
        
        if quality_metrics["overall_confidence"] >= 0.8:
            return "passed"
        elif quality_metrics["overall_confidence"] >= 0.6:
            return "warning"
        else:
            return "failed"

    def _apply_quality_filters(self, qa_pairs: List[QAMetadata]) -> List[QAMetadata]:
        """Apply quality filters to remove low-quality Q&A pairs."""
        
        filtered_pairs = []
        
        for qa in qa_pairs:
            self.logger.info(f"Quality check - Extractive: {qa.extractive_score:.3f}, "
                           f"Factual: {qa.factual_accuracy:.3f}, "
                           f"Cultural: {qa.cultural_sensitivity:.3f}, "
                           f"Uniqueness: {qa.uniqueness_score:.3f}")
            
            # Apply quality thresholds
            if (qa.extractive_score >= self.quality_thresholds["min_extractive_score"] and
                qa.factual_accuracy >= self.quality_thresholds["min_factual_accuracy"] and
                qa.cultural_sensitivity >= self.quality_thresholds["min_cultural_sensitivity"] and
                qa.uniqueness_score >= self.quality_thresholds["min_uniqueness_score"]):
                
                filtered_pairs.append(qa)
                self.logger.info("  âœ… Passed quality check")
            else:
                self.logger.info("  âŒ Failed quality check")
        
        self.logger.info(f"Quality filtering: {len(filtered_pairs)}/{len(qa_pairs)} pairs passed")
        
        return filtered_pairs

    def _balance_coverage(self, qa_pairs: List[QAMetadata], target_size: int) -> List[QAMetadata]:
        """Balance coverage across universities, personas, and question types."""
        
        # Group by categories
        by_university = defaultdict(list)
        by_persona = defaultdict(list)
        by_question_type = defaultdict(list)
        
        for qa in qa_pairs:
            by_university[qa.university].append(qa)
            by_persona[qa.student_persona].append(qa)
            # Extract question type from generation method
            q_type = qa.generation_method.split('_')[1] if '_' in qa.generation_method else "general"
            by_question_type[q_type].append(qa)
        
        # Calculate target distribution
        universities = list(by_university.keys())
        personas = list(by_persona.keys())
        question_types = list(by_question_type.keys())
        
        balanced_pairs = []
        
        # Distribute evenly across categories
        pairs_per_university = target_size // len(universities) if universities else target_size
        
        for university in universities:
            available_pairs = by_university[university]
            
            # Sort by quality (highest first)
            available_pairs.sort(key=lambda x: x.confidence_level, reverse=True)
            
            # Take top quality pairs
            selected = available_pairs[:pairs_per_university]
            balanced_pairs.extend(selected)
        
        # Fill remaining slots with highest quality pairs
        remaining_slots = target_size - len(balanced_pairs)
        if remaining_slots > 0:
            all_remaining = [qa for qa in qa_pairs if qa not in balanced_pairs]
            all_remaining.sort(key=lambda x: x.confidence_level, reverse=True)
            balanced_pairs.extend(all_remaining[:remaining_slots])
        
        self.logger.info(f"Balanced coverage: Selected {len(balanced_pairs)} pairs from {len(qa_pairs)} available")
        
        return balanced_pairs[:target_size]

    def _create_final_dataset(self, qa_pairs: List[QAMetadata]) -> List[Dict[str, Any]]:
        """Create final dataset in specified format."""
        
        dataset = []
        
        for qa in qa_pairs:
            # Convert to dictionary format
            qa_dict = {
                "question": qa.question,
                "answer": qa.answer,
                "context": qa.context,
                "university": qa.university,
                "audience": qa.audience,
                "answer_type": qa.answer_type,
                "tone": qa.tone,
                "confidence_level": qa.confidence_level,
                "source_file": qa.source_file,
                
                # Extended metadata
                "metadata": {
                    "student_persona": qa.student_persona,
                    "question_complexity": qa.question_complexity,
                    "financial_details": qa.financial_details,
                    "grade_calculation": qa.grade_calculation,
                    "multi_university": qa.multi_university,
                    "bengali_integration": qa.bengali_integration,
                    "actionable_guidance": qa.actionable_guidance,
                    "difficulty_level": qa.difficulty_level,
                    "expected_response_time": qa.expected_response_time,
                    "requires_calculation": qa.requires_calculation,
                    "requires_verification": qa.requires_verification
                },
                
                # Quality metrics
                "quality": {
                    "extractive_score": qa.extractive_score,
                    "factual_accuracy": qa.factual_accuracy,
                    "cultural_sensitivity": qa.cultural_sensitivity,
                    "uniqueness_score": qa.uniqueness_score,
                    "validation_status": qa.validation_status
                },
                
                # Source tracking
                "source_info": {
                    "paragraph_source": qa.paragraph_source,
                    "generation_method": qa.generation_method,
                    "creation_timestamp": qa.creation_timestamp
                }
            }
            
            dataset.append(qa_dict)
        
        return dataset

    async def _save_dataset_outputs(self, dataset: List[Dict[str, Any]], output_path: str):
        """Save dataset and related outputs."""
        
        # Create output directories
        output_dir = Path(output_path).parent
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # Save main dataset (JSONL format)
        jsonl_path = output_path
        with open(jsonl_path, 'w', encoding='utf-8') as f:
            for item in dataset:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')
        
        # Save preview dataset (human-readable JSON)
        preview_dir = output_dir / "preview"
        preview_dir.mkdir(exist_ok=True)
        
        preview_path = preview_dir / f"{Path(output_path).stem}_preview.json"
        with open(preview_path, 'w', encoding='utf-8') as f:
            json.dump(dataset[:10], f, indent=2, ensure_ascii=False)  # First 10 for preview
        
        # Save validation report
        validation_path = output_dir / f"{Path(output_path).stem}_validation.json"
        validation_report = self._generate_validation_report(dataset)
        
        with open(validation_path, 'w', encoding='utf-8') as f:
            json.dump(validation_report, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"Saved dataset: {jsonl_path}")
        self.logger.info(f"Saved preview: {preview_path}")
        self.logger.info(f"Saved validation: {validation_path}")

    def _generate_validation_report(self, dataset: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Generate comprehensive validation report."""
        
        report = {
            "total_pairs": len(dataset),
            "validation_summary": {
                "passed": 0,
                "warning": 0,
                "failed": 0
            },
            "quality_metrics": {
                "avg_extractive_score": 0,
                "avg_factual_accuracy": 0,
                "avg_cultural_sensitivity": 0,
                "avg_uniqueness_score": 0,
                "avg_confidence_level": 0
            },
            "coverage_analysis": {
                "universities": defaultdict(int),
                "personas": defaultdict(int),
                "question_types": defaultdict(int),
                "difficulty_levels": defaultdict(int)
            },
            "validation_timestamp": datetime.now().isoformat(),
            "data_issues": []
        }
        
        # Calculate metrics
        quality_sums = {
            "extractive_score": 0,
            "factual_accuracy": 0,
            "cultural_sensitivity": 0,
            "uniqueness_score": 0,
            "confidence_level": 0
        }
        
        for item in dataset:
            # Validation status
            status = item["quality"]["validation_status"]
            report["validation_summary"][status] += 1
            
            # Quality metrics
            quality = item["quality"]
            quality_sums["extractive_score"] += quality["extractive_score"]
            quality_sums["factual_accuracy"] += quality["factual_accuracy"]
            quality_sums["cultural_sensitivity"] += quality["cultural_sensitivity"]
            quality_sums["uniqueness_score"] += quality["uniqueness_score"]
            quality_sums["confidence_level"] += item["confidence_level"]
            
            # Coverage analysis
            report["coverage_analysis"]["universities"][item["university"]] += 1
            report["coverage_analysis"]["personas"][item["metadata"]["student_persona"]] += 1
            report["coverage_analysis"]["difficulty_levels"][str(item["metadata"]["difficulty_level"])] += 1
            
            # Check for data issues
            if len(item["question"]) < 10:
                report["data_issues"].append(f"Very short question: {item['question'][:50]}...")
            
            if len(item["answer"]) < 50:
                report["data_issues"].append(f"Very short answer for: {item['question'][:50]}...")
        
        # Calculate averages
        total = len(dataset)
        if total > 0:
            for metric in quality_sums:
                report["quality_metrics"][f"avg_{metric}"] = quality_sums[metric] / total
        
        return report

    def _generate_generation_stats(self, dataset: List[Dict[str, Any]], txt_files: List[Path]) -> Dict[str, Any]:
        """Generate comprehensive generation statistics."""
        
        return {
            "generation_summary": {
                "total_qa_pairs": len(dataset),
                "source_files_processed": len(txt_files),
                "avg_pairs_per_file": len(dataset) / len(txt_files) if txt_files else 0,
                "generation_time": datetime.now().isoformat()
            },
            "source_file_breakdown": dict(self.source_file_stats),
            "university_distribution": dict(self.university_coverage),
            "persona_distribution": dict(self.persona_coverage),
            "quality_distribution": {
                "high_quality": sum(1 for item in dataset if item["confidence_level"] >= 0.8),
                "medium_quality": sum(1 for item in dataset if 0.6 <= item["confidence_level"] < 0.8),
                "low_quality": sum(1 for item in dataset if item["confidence_level"] < 0.6)
            },
            "feature_analysis": {
                "with_calculations": sum(1 for item in dataset if item["metadata"]["requires_calculation"]),
                "multi_university": sum(1 for item in dataset if item["metadata"]["multi_university"]),
                "bengali_integration": sum(1 for item in dataset if item["metadata"]["bengali_integration"]),
                "actionable_guidance": sum(1 for item in dataset if item["metadata"]["actionable_guidance"])
            }
        }


# CLI Interface
async def main():
    """Main CLI interface for production TXT dataset generator."""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Production-Grade TXT Dataset Generator for Mistral 7B Fine-tuning"
    )
    parser.add_argument("input_dir", help="Directory containing .txt files")
    parser.add_argument("output_path", help="Output path for generated dataset (.jsonl)")
    parser.add_argument("--size", type=int, default=1000, help="Target dataset size")
    parser.add_argument("--config", default="config/config.yaml", help="Configuration file")
    
    args = parser.parse_args()
    
    # Initialize generator
    generator = ProductionTxtDatasetGenerator(args.config)
    
    # Process files
    result = await generator.process_txt_files(
        args.input_dir, 
        args.output_path, 
        args.size
    )
    
    # Print results
    print("\nâœ… Dataset Generation Complete!")
    print(f"ðŸ“Š Generated {result['total_pairs']} Q&A pairs")
    print(f"ðŸ“ Dataset: {result['dataset_path']}")
    print(f"ðŸ“‹ Source files processed: {result['source_files_processed']}")
    print(f"â­ Statistics: {json.dumps(result['statistics']['quality_distribution'], indent=2)}")


if __name__ == "__main__":
    asyncio.run(main())
