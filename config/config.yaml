# SetForge Configuration File
# Customize settings for your dataset generation needs

# Progress tracking and resumability configuration
progress:
  checkpoint_interval_files: 5      # Save checkpoint every N files
  checkpoint_interval_minutes: 10   # Or every N minutes
  status_refresh_rate: 2.0          # Dashboard refresh rate in seconds
  enable_live_dashboard: true       # Enable real-time status dashboard
  save_partial_results: true        # Save results incrementally
  auto_cleanup_checkpoints: true    # Auto-remove old checkpoints
  max_checkpoint_files: 10          # Maximum checkpoint files to keep

# Quality monitoring and alerts configuration
quality:
  alert_threshold: 0.8              # Alert when quality drops below this
  quality_report_interval: 100      # Generate report every N QA pairs
  enable_quality_trends: true       # Track quality trends over time
  track_validation_details: true    # Track detailed validation metrics
  trend_window_minutes: 10          # Time window for trend analysis
  min_samples_for_trend: 10         # Minimum samples needed for trend
  excellent_threshold: 0.9          # Threshold for excellent quality
  good_threshold: 0.8               # Threshold for good quality
  fair_threshold: 0.7               # Threshold for fair quality
  poor_threshold: 0.0               # Threshold for poor quality

# Input file handling configuration
input:
  valid_extensions: [".txt", ".md"] # Valid file extensions to process
  min_file_size_bytes: 100          # Minimum file size to process
  max_file_size_bytes: 10485760     # Maximum file size (10MB)
  encoding: "utf-8"                 # File encoding
  skip_empty_files: true            # Skip empty files

# Text chunking configuration - OPTIMIZED FOR MAXIMUM QnA GENERATION
chunking:
  max_chunk_size: 800               # Reduced for finer-grained chunks
  min_chunk_size: 200               # Smaller minimum for more chunks
  overlap_size: 100                 # Reduced overlap for more unique content
  chunk_by_sections: true           # Split by markdown headers
  section_headers: ["#", "##", "###", "####", "•", "-", "·"]  # Added bullet points
  paragraph_split: true             # Also split by paragraphs
  preserve_structure: true          # Maintain markdown formatting
  sentence_split: true              # NEW: Split by sentences for finer chunks
  bullet_split: true                # NEW: Split by bullet points/lists
  max_sentences_per_chunk: 4        # NEW: Limit sentences per chunk
  enable_micro_chunks: true         # NEW: Create very small chunks for dense QA generation

# LLM API configuration
llm:
  provider: "digitalocean"
  base_url: "https://inference.do-ai.run"
  api_key: ""                   # Set via DIGITALOCEAN_API_KEY env var
  model_name: "llama3-8b-instruct"
  max_tokens: 1024
  temperature: 0.1              # Low temperature for consistent output
  timeout: 30
  max_retries: 3
  retry_delay: 1.0

# QA generation configuration - OPTIMIZED FOR HIGH VOLUME
qa:
  questions_per_chunk: 6        # Optimized from 8 to 6 for target range
  min_question_length: 8        # Slightly reduced minimum
  max_question_length: 250      # Increased maximum for complex questions
  min_answer_length: 3          # Reduced minimum for short factual answers
  max_answer_length: 600        # Increased for detailed answers
  question_types: ["factual", "definition", "process", "comparison", "list", "why", "how", "what", "when", "where"]  # Expanded types
  forbidden_patterns: ["in my opinion", "i think", "probably", "might be", "could be", "generally", "typically"]
  enable_paraphrasing: true     # NEW: Enable question paraphrasing for data augmentation
  paraphrases_per_question: 2   # NEW: Generate 2 paraphrased versions per question
  max_total_questions: 15       # NEW: Maximum questions if paraphrasing enabled
  enable_multi_question: true   # NEW: Multiple questions for same answer when appropriate

# Validation configuration - ENHANCED FOR DEDUPLICATION
validation:
  min_relevancy_score: 0.70     # Slightly reduced for more permissive but still high quality
  min_source_overlap: 0.65      # Reduced slightly to allow more variety
  max_inference_ratio: 0.10     # More strict to prevent hallucinations
  embedding_model: "sentence-transformers/all-MiniLM-L6-v2"
  enable_semantic_validation: true
  enable_extractive_validation: true
  enable_hallucination_detection: true
  enable_deduplication: true    # NEW: Enable deduplication
  similarity_threshold: 0.85    # NEW: Threshold for near-duplicate detection
  max_similar_questions: 2      # NEW: Max similar questions allowed
  enable_answer_deduplication: true  # NEW: Also deduplicate by answer similarity

# Cost optimization configuration - SCALED FOR HIGH VOLUME
cost:
  max_total_cost_usd: 150.0     # Increased budget for 10K+ QA pairs
  cost_per_1k_tokens: 0.0005    # DigitalOcean pricing
  enable_caching: true
  cache_dir: ".setforge_cache"
  batch_size: 8                 # Increased batch size for efficiency
  enable_async: true
  max_concurrent_requests: 12   # NEW: Higher concurrency for faster processing
  enable_cost_monitoring: true  # NEW: Enhanced cost tracking
  cost_alert_threshold: 100.0   # NEW: Alert when approaching budget limit

# Output configuration
output:
  output_file: "output/datasets/qa_dataset.jsonl"  # Updated path
  include_metadata: true
  pretty_print: false

# General settings
log_level: "INFO"               # DEBUG, INFO, WARNING, ERROR
log_file: null                  # Optional log file path
debug_mode: false
dry_run: false                  # Test without API calls
